{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd99144-e212-466e-80b5-def469ee59af",
   "metadata": {},
   "source": [
    "## TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7fe8e-7b06-464b-847e-165cf4b8a761",
   "metadata": {},
   "source": [
    "### 1. Describe the purpose and benefits of pooling in CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ca198-9c6f-42f6-a740-f7582434f44a",
   "metadata": {},
   "source": [
    "Ans--> Pooling, also known as subsampling or downsampling, is a crucial operation in Convolutional Neural Networks (CNNs) used to reduce the spatial dimensions of the feature maps while retaining their important information. The primary purpose of pooling is to progressively downscale the feature maps, making the network more computationally efficient, reducing memory requirements, and extracting the most relevant features from the input data.\n",
    "\n",
    "The benefits of pooling in CNNs include:\n",
    "\n",
    "1. Spatial invariance: Pooling helps achieve translation invariance, which means the network can recognize patterns and features regardless of their exact spatial position in the input data. This is crucial for tasks like image recognition, where the object of interest can appear at different positions within the image.\n",
    "\n",
    "2. Dimensionality reduction: As the CNN progresses through its layers, the spatial dimensions of the feature maps decrease due to pooling operations. This reduction helps reduce the computational load and the number of parameters in the network, making it more manageable and faster to train.\n",
    "\n",
    "3. Information retention: Pooling aims to retain the most important features while discarding less relevant details. By downsampling the feature maps, the network focuses on the dominant patterns, edges, and textures, which are critical for the classification task.\n",
    "\n",
    "4. Overfitting prevention: Pooling can help prevent overfitting by reducing the spatial resolution of the feature maps and providing a form of regularization. With fewer parameters to learn, the network becomes less prone to memorizing noise in the training data.\n",
    "\n",
    "5. Increased receptive field: Pooling effectively increases the receptive field of neurons in deeper layers. It allows the network to capture more global features and context from the input data, leading to better generalization and performance.\n",
    "\n",
    "There are different types of pooling operations, such as max pooling and average pooling. Max pooling takes the maximum value from a specific window of the feature map, while average pooling computes the average value within the window. Both methods are effective at reducing spatial dimensions and extracting essential information, but max pooling is more commonly used as it tends to preserve more prominent features and has been shown to work well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677dae12-7816-47ce-a41e-28c218cfd8fb",
   "metadata": {},
   "source": [
    "### 2. Explain the difference between min pooling and max pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac9fd2-515a-4015-bf9a-9a084c256312",
   "metadata": {},
   "source": [
    "Ans--> Max pooling and min pooling are both types of pooling operations used in Convolutional Neural Networks (CNNs) for downscaling the feature maps. However, they differ in their behavior and how they select values from the input feature maps:\n",
    "\n",
    "1. Max Pooling:\n",
    "   - Max pooling takes the maximum value from a specific window (also called the pooling kernel or filter) of the feature map.\n",
    "   - The window slides over the input feature map, and at each position, it selects the maximum value within the window.\n",
    "   - The selected maximum value is then used to represent the corresponding region in the downsampled feature map.\n",
    "   - Max pooling is popular because it retains the most prominent features, such as edges and corners, which are crucial for image recognition tasks.\n",
    "   - It provides a form of translation invariance, as the maximum value represents the most significant feature within the window, regardless of its exact position.\n",
    "   - Max pooling tends to be more robust against noise in the data, as it focuses on the most relevant information.\n",
    "\n",
    "2. Min Pooling:\n",
    "   - Min pooling, on the other hand, takes the minimum value from the pooling window of the feature map.\n",
    "   - Similar to max pooling, the window slides over the feature map, and at each position, it selects the minimum value within the window.\n",
    "   - The selected minimum value is used to represent the corresponding region in the downsampled feature map.\n",
    "   - Unlike max pooling, which emphasizes prominent features, min pooling emphasizes the least intense features in the data.\n",
    "   - Min pooling is not commonly used in practice for image recognition tasks, as it tends to retain less important information and may not be as effective as max pooling.\n",
    "\n",
    "In summary, max pooling selects the maximum value from the pooling window, while min pooling selects the minimum value. Max pooling is widely used in CNN architectures and is known for preserving important features and improving the network's performance, whereas min pooling is less common and may not be as suitable for typical image recognition tasks due to its emphasis on less intense features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1aa2f-2a0f-49ac-95d7-4719d8ff3660",
   "metadata": {},
   "source": [
    "### 3. Discuss the concept of padding in CNN and its significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097f86e-f2b4-46d8-9daf-9f6d1c9ef8a1",
   "metadata": {},
   "source": [
    "Ans--> Padding in Convolutional Neural Networks (CNNs) refers to the process of adding extra pixels (often with zero values) around the borders of an input feature map before applying convolutional or pooling operations. Padding helps retain the spatial dimensions of the feature maps after these operations, and it has significant implications for the network's architecture and performance. There are two common types of padding:\n",
    "\n",
    "1. Valid Padding (No Padding):\n",
    "   - In this type of padding, no extra pixels are added to the input feature map. The filter is applied only to the valid positions where the filter and the input feature map fully overlap.\n",
    "   - As a result, the output feature map's spatial dimensions are reduced after convolution or pooling operations, and information at the borders of the input map is lost.\n",
    "   - While valid padding reduces the computational cost and memory requirements, it may lead to a reduction in feature map dimensions, which can result in a smaller receptive field for deeper layers, potentially limiting the network's ability to capture global features and context.\n",
    "\n",
    "2. Same Padding:\n",
    "   - In same padding, the necessary number of extra pixels is added to the input feature map so that the output feature map has the same spatial dimensions as the input.\n",
    "   - If the filter size is odd, the same number of pixels is added on all sides. If the filter size is even, more pixels are added to the right and bottom sides to maintain an equal spatial dimension.\n",
    "   - Same padding ensures that the convolutional and pooling operations do not reduce the spatial dimensions of the feature maps, allowing information at the borders to be preserved.\n",
    "   - With same padding, the network can retain more context and capture global features effectively, leading to better performance in tasks where spatial information is essential, such as image segmentation and object detection.\n",
    "\n",
    "The significance of padding in CNNs includes:\n",
    "\n",
    "1. Preserving spatial information: Padding helps prevent a progressive reduction in spatial dimensions, ensuring that the network maintains important information from the input, especially at the borders of the feature maps.\n",
    "\n",
    "2. Enabling deeper architectures: By preserving spatial dimensions through padding, deeper layers can have larger receptive fields, allowing the network to learn more complex patterns and global context.\n",
    "\n",
    "3. Handling edge effects: Without padding, the filter's center might not align with the edge pixels of the input feature map, leading to incomplete information. Padding resolves this issue by allowing the filter to be applied to all input pixels.\n",
    "\n",
    "4. Efficient information flow: With same padding, the information flow through the network is more efficient since the output size remains consistent, and the network can better propagate gradients during training.\n",
    "\n",
    "In conclusion, padding plays a crucial role in CNNs by maintaining spatial information, enabling deeper architectures, and improving the network's ability to capture context and features effectively, ultimately contributing to better performance in various computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d530ab07-1379-4513-b255-b4a2c43477a1",
   "metadata": {},
   "source": [
    "### 4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449b552-33cc-4839-861d-aeab780ee98b",
   "metadata": {},
   "source": [
    "Ans--> Zero-padding and valid-padding are two types of padding techniques used in Convolutional Neural Networks (CNNs) that have different effects on the output feature map size after convolutional or pooling operations. Let's compare and contrast them in terms of their impact on the output feature map size:\n",
    "\n",
    "1. Zero-padding:\n",
    "   - Zero-padding involves adding extra pixels (often with zero values) around the borders of the input feature map before applying convolution or pooling operations.\n",
    "   - The amount of zero-padding is determined by the size of the convolutional or pooling filter and the desired output size.\n",
    "   - With zero-padding, the output feature map size can remain the same (if desired) or be increased compared to the input feature map size, depending on the padding size and the stride used during the operation.\n",
    "   - The formula to compute the output feature map size with zero-padding is:\n",
    "     Output Size = (Input Size + 2 * Padding Size - Filter Size) / Stride + 1\n",
    "   - Zero-padding is commonly used when we want to maintain the spatial dimensions of the feature maps or when we need to ensure that the filter is applied to all input pixels, even at the borders.\n",
    "\n",
    "2. Valid-padding (No Padding):\n",
    "   - Valid-padding, also known as no padding, involves not adding any extra pixels around the borders of the input feature map before convolution or pooling operations.\n",
    "   - The filter is applied only to the valid positions where the filter and the input feature map fully overlap.\n",
    "   - As a result, the output feature map size is reduced compared to the input feature map size, and the spatial dimensions of the feature maps are progressively reduced as we move deeper into the network.\n",
    "   - The formula to compute the output feature map size with valid-padding is:\n",
    "     Output Size = (Input Size - Filter Size) / Stride + 1\n",
    "   - Valid-padding is commonly used when we want to reduce the computational cost and memory requirements of the network, at the cost of losing some spatial information at the borders of the feature maps.\n",
    "\n",
    "Comparison:\n",
    "- Zero-padding can maintain or increase the output feature map size, while valid-padding reduces the output feature map size.\n",
    "- Zero-padding helps to preserve information at the borders of the input feature map, while valid-padding discards this information.\n",
    "- Both padding techniques are useful in different situations. Zero-padding is preferred when maintaining spatial dimensions and preserving information is critical, while valid-padding is used to reduce computational complexity and memory requirements.\n",
    "\n",
    "Contrast:\n",
    "- Zero-padding involves adding extra pixels, while valid-padding does not add any extra pixels (no padding).\n",
    "- Zero-padding retains spatial information, whereas valid-padding loses spatial information due to the reduction in feature map size.\n",
    "- Zero-padding is more computationally expensive compared to valid-padding, as it involves processing extra pixels in the convolution or pooling operations.\n",
    "\n",
    "In summary, the choice between zero-padding and valid-padding depends on the specific requirements of the CNN architecture and the task at hand. Zero-padding is favored when spatial information preservation is crucial, while valid-padding is used to reduce computation and memory overhead, sacrificing some spatial information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e25e9-4aa2-4366-b81f-d90f9bb488df",
   "metadata": {},
   "source": [
    "## TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e0006b-14ea-446f-beea-ad56fe857b17",
   "metadata": {},
   "source": [
    "### 1. Provide a brief overview of LeNet-5 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d673aa4d-ef76-48b6-a22f-b2629bb20fce",
   "metadata": {},
   "source": [
    "Ans--> LeNet-5 is a pioneering Convolutional Neural Network (CNN) architecture developed by Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner in 1998. It was designed for handwritten digit recognition, specifically for the task of recognizing handwritten digits in the MNIST dataset. LeNet-5 played a significant role in popularizing CNNs and has been influential in the development of modern deep learning architectures.\n",
    "\n",
    "The LeNet-5 architecture consists of the following key components:\n",
    "\n",
    "1. Input Layer:\n",
    "   - The network takes grayscale images of size 32x32 pixels as input. The images are passed through the network in their original resolution.\n",
    "\n",
    "2. Convolutional Layers:\n",
    "   - LeNet-5 has two sets of convolutional layers. The first convolutional layer has six filters of size 5x5, and the second convolutional layer has 16 filters of size 5x5.\n",
    "   - The convolutional layers apply these filters to the input images to extract different features, such as edges and patterns.\n",
    "\n",
    "3. Activation Function:\n",
    "   - After each convolutional layer, a sigmoid activation function is applied element-wise to introduce non-linearity into the network.\n",
    "\n",
    "4. Pooling Layers:\n",
    "   - LeNet-5 uses average pooling layers after the first and second convolutional layers. Each pooling layer has a 2x2 window and a stride of 2, resulting in a downsampling of the feature maps.\n",
    "   - Pooling helps reduce the spatial dimensions of the feature maps and helps make the network more computationally efficient.\n",
    "\n",
    "5. Fully Connected Layers:\n",
    "   - After the convolutional and pooling layers, LeNet-5 has three fully connected layers. The first fully connected layer has 120 neurons, the second has 84 neurons, and the final output layer has 10 neurons (corresponding to the 10 possible digits 0 to 9).\n",
    "   - The output layer uses the softmax activation function to produce a probability distribution over the 10 classes, representing the predicted probability of each digit.\n",
    "\n",
    "6. Training:\n",
    "   - LeNet-5 is typically trained using the stochastic gradient descent (SGD) optimization algorithm and the backpropagation algorithm to update the network's weights during training.\n",
    "   - The network is trained to minimize the cross-entropy loss between the predicted probabilities and the actual labels.\n",
    "\n",
    "LeNet-5's architecture is relatively simple compared to modern CNNs, but it demonstrated the effectiveness of using convolutional layers, pooling layers, and non-linear activation functions in image recognition tasks. It achieved impressive results on the MNIST dataset and paved the way for more complex and deep CNN architectures that are widely used today for a wide range of computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad1f19-fc11-46a5-a163-1d43ab8be046",
   "metadata": {},
   "source": [
    "## 2. Describe the key components of LeNet-5 and their respective purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce9114-3c31-47ad-b340-cdcfd8b84451",
   "metadata": {},
   "source": [
    "Ans--> LeNet-5 is a classic Convolutional Neural Network (CNN) architecture designed for handwritten digit recognition. It consists of several key components, each serving a specific purpose in the network's overall functionality. Let's describe these components and their respective purposes:\n",
    "\n",
    "1. Input Layer:\n",
    "   - The input layer receives the grayscale images of size 32x32 pixels. Each pixel represents the intensity value of the corresponding pixel in the handwritten digit image.\n",
    "   - The input layer passes the image data through the network, initiating the flow of information and computations.\n",
    "\n",
    "2. Convolutional Layers:\n",
    "   - LeNet-5 contains two sets of convolutional layers: C1 and C3.\n",
    "   - C1: The first convolutional layer consists of six filters of size 5x5. These filters are applied to the input images to detect different features, such as edges and patterns.\n",
    "   - C3: The second convolutional layer has 16 filters of size 5x5. It takes the output from C1 as input and extracts more complex features from the learned representations in C1.\n",
    "\n",
    "3. Activation Function:\n",
    "   - After each convolutional layer (C1 and C3), a sigmoid activation function is applied element-wise to introduce non-linearity into the network.\n",
    "   - The activation function allows the network to model complex relationships between the learned features and the input data, making the network capable of capturing non-linear patterns.\n",
    "\n",
    "4. Pooling Layers:\n",
    "   - LeNet-5 uses average pooling layers (P2 and P4) after the first and second convolutional layers.\n",
    "   - P2: The first pooling layer follows C1 and has a 2x2 window with a stride of 2. It downsamples the feature maps, reducing their spatial dimensions and extracting the most important information.\n",
    "   - P4: The second pooling layer follows C3 and also has a 2x2 window with a stride of 2, further reducing the spatial dimensions and making the network more computationally efficient.\n",
    "\n",
    "5. Fully Connected Layers:\n",
    "   - After the convolutional and pooling layers, LeNet-5 has three fully connected layers: F5, F6, and the output layer.\n",
    "   - F5: The first fully connected layer has 120 neurons. It takes the output from the last pooling layer (P4) and performs a dense transformation to further capture high-level features and patterns.\n",
    "   - F6: The second fully connected layer consists of 84 neurons. It continues to extract more abstract representations from the previous layer.\n",
    "   - Output Layer: The final layer of LeNet-5 is a fully connected layer with 10 neurons, each corresponding to one of the possible digit classes (0 to 9). The output layer uses the softmax activation function to produce a probability distribution over the classes, representing the predicted probabilities for each digit class.\n",
    "\n",
    "The overall purpose of these key components is to process and transform the input data through convolutional layers, pooling layers, and fully connected layers to learn meaningful features and patterns. The learned representations are then used to classify the input handwritten digits into their respective classes with high accuracy. The architecture demonstrated the effectiveness of using convolutional layers, pooling layers, and non-linear activation functions, laying the foundation for more advanced CNN architectures in modern deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742b4ae-2bd5-4506-9710-a64ca2ea5929",
   "metadata": {},
   "source": [
    "## 3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712cf14f-a8b3-4149-828e-b6f47c9aba8b",
   "metadata": {},
   "source": [
    "Ans--> Advantages of LeNet-5 in Image Classification Tasks:\n",
    "\n",
    "1. Simplicity: LeNet-5 has a relatively simple architecture compared to modern CNNs, making it easy to understand, implement, and train. Its simplicity was one of the key factors that helped popularize CNNs and deep learning.\n",
    "\n",
    "2. Effective Feature Extraction: LeNet-5 employs convolutional layers, which are effective at learning hierarchical features from the input images. These layers can automatically detect edges, corners, and other low-level features, which are essential for recognizing patterns in images.\n",
    "\n",
    "3. Translation Invariance: The pooling layers in LeNet-5 help create translation invariance, meaning the network can recognize patterns and features regardless of their exact spatial position in the input images. This property is crucial for image recognition tasks.\n",
    "\n",
    "4. Efficient Computation: LeNet-5 uses average pooling and relatively small filter sizes, reducing the computational complexity and memory requirements compared to deeper networks. This efficiency makes it suitable for training on less powerful hardware.\n",
    "\n",
    "5. Suitable for Small Images: LeNet-5 was originally designed for handwritten digit recognition on the MNIST dataset, which consists of small, grayscale images (28x28 pixels). It performs well on similar small-scale image classification tasks.\n",
    "\n",
    "Limitations of LeNet-5 in Image Classification Tasks:\n",
    "\n",
    "1. Limited Depth: LeNet-5 has a shallow architecture compared to modern CNNs. Deeper architectures with many layers have shown to be more effective at learning complex features, and they are better suited for more challenging and large-scale image classification tasks.\n",
    "\n",
    "2. Suboptimal for Complex Data: While LeNet-5 performs well on simple datasets like MNIST, it may struggle with more complex and diverse datasets like ImageNet, which contains high-resolution color images with various objects in cluttered scenes.\n",
    "\n",
    "3. Sensitive to Input Size: The original LeNet-5 architecture is designed for 32x32 pixel input images. If the input images are larger or have different aspect ratios, it may require modifications to the network architecture or introduce padding, which could affect performance.\n",
    "\n",
    "4. Limited Activation Function: LeNet-5 uses sigmoid activation functions, which have vanishing gradient problems during training. Modern CNNs often use more robust activation functions like ReLU (Rectified Linear Unit) or its variants.\n",
    "\n",
    "5. Overfitting: Due to its smaller size and limited data augmentation techniques, LeNet-5 may be more prone to overfitting, especially when applied to larger and more complex datasets. Regularization techniques may be necessary to mitigate this issue.\n",
    "\n",
    "In summary, LeNet-5 was a pioneering CNN architecture that laid the groundwork for modern deep learning techniques. While it has its advantages, such as simplicity and efficient computation, it is limited by its shallow architecture and suboptimal performance on complex image classification tasks. For more challenging tasks, researchers have developed deeper and more sophisticated CNN architectures that outperform LeNet-5 in terms of accuracy and generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942fca3e-90ca-4d14-bb9e-9f38c29dbdbd",
   "metadata": {},
   "source": [
    "### 4. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426076f7-5fc0-4191-8f63-0e81306701ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.33.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.56.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436da0bd-6279-4983-8289-c01abc173851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 07:24:51.995228: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-10 07:24:52.589774: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-10 07:24:52.592309: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 07:24:54.504876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101e4a01-3b84-4547-a21b-6bcc301a702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66aa16ec-8352-439f-8e76-f7de3b79ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mnist datasets\n",
    "(X_train,y_train),(X_test,y_test)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb47c80-ef7f-4b4c-87e1-3d05734f0c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing datasets\n",
    "X_train=X_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "X_test=X_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "y_train=keras.utils.to_categorical(y_train,10)\n",
    "y_test=keras.utils.to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cbc63e7-aa74-4dee-af29-eb79fd46cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the lenet-5 model\n",
    "model=models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2832061d-5f21-4a2c-b241-ee54a313e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Conv2D(6,(5,5),activation='relu',input_shape=(28,28,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c60ac62a-4406-4c07-bd3f-58bc0639a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(16,(5,5),activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(120,activation='relu'))\n",
    "model.add(layers.Dense(84,activation='relu'))\n",
    "model.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84effa1-64ef-40f5-af03-024b744a4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db193cf-5faa-48c9-9f6a-f0dd5aeeaf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.2302 - accuracy: 0.9315 - val_loss: 0.0805 - val_accuracy: 0.9732\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.0722 - accuracy: 0.9781 - val_loss: 0.0510 - val_accuracy: 0.9818\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.0528 - accuracy: 0.9834 - val_loss: 0.0491 - val_accuracy: 0.9831\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.0417 - accuracy: 0.9867 - val_loss: 0.0426 - val_accuracy: 0.9858\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.0336 - accuracy: 0.9892 - val_loss: 0.0462 - val_accuracy: 0.9856\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.0278 - accuracy: 0.9911 - val_loss: 0.0338 - val_accuracy: 0.9902\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.0249 - accuracy: 0.9918 - val_loss: 0.0371 - val_accuracy: 0.9875\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.0216 - accuracy: 0.9927 - val_loss: 0.0347 - val_accuracy: 0.9896\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.0181 - accuracy: 0.9937 - val_loss: 0.0368 - val_accuracy: 0.9905\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.0151 - accuracy: 0.9952 - val_loss: 0.0335 - val_accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "# Train the models\n",
    "\n",
    "history=model.fit(X_train,y_train, epochs=10, batch_size=64,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e53826d4-dc26-40dc-a371-f3f28283e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0335 - accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy=model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7370c12-4c97-488e-af24-c4ec43a01e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABl+klEQVR4nO3dd3hUddrG8e9kMqmkQUhICJAAAqFLL4qyKoiAsGsBV1GW4qKiIroqKhZWxbKUtRAFCSKooGt5WcUSUVeaRlCaYGhCIIUQIIWEtJl5/5jMwJBQAklOyv25rrlIzpw585wEmdtfNdntdjsiIiIi9YiH0QWIiIiIVDcFIBEREal3FIBERESk3lEAEhERkXpHAUhERETqHQUgERERqXcUgERERKTe8TS6gJrIZrORmppKQEAAJpPJ6HJERETkPNjtdnJzc4mMjMTD4+xtPApA5UhNTaVZs2ZGlyEiIiIX4MCBA0RFRZ31HAWgcgQEBACOH2BgYKDB1YiIiMj5yMnJoVmzZq7P8bNRACqHs9srMDBQAUhERKSWOZ/hKxoELSIiIvWOApCIiIjUOwpAIiIiUu9oDNBFsFqtFBcXG12GSKWzWCyYzWajyxARqTIKQBfAbreTnp5OVlaW0aWIVJng4GCaNGmitbBEpE5SALoAzvATFhaGn5+fPiCkTrHb7eTn55ORkQFARESEwRWJiFQ+BaAKslqtrvDTqFEjo8sRqRK+vr4AZGRkEBYWpu4wEalzNAi6gpxjfvz8/AyuRKRqOf+Oa5ybiNRFCkAXSN1eUtfp77iI1GUKQCIiIlLvKACJiIhIvaMAJBflyiuvZMqUKed9/r59+zCZTGzatKnKahIRETkXBaB6wmQynfUxduzYC7ruxx9/zD//+c/zPr9Zs2akpaXRsWPHC3q/CzFo0CDMZjM//vhjtb2niIiUz263k5FTwL7MPEPr0DT4eiItLc319fLly3nyySdJSkpyHXNOe3YqLi7GYrGc87oNGzasUB1ms5kmTZpU6DUXIzk5mfXr1zN58mQWLlxInz59qu29y3O+P1cRkdquxGpj/9F89mQcZ/fh4+zJyGPP4ePsOXyc3IISBrRpzDvjehlWn1qAKoHdbie/qMSQh91uP68amzRp4noEBQVhMplc3xcUFBAcHMwHH3zAlVdeiY+PD0uXLuXIkSPccsstREVF4efnR6dOnXj//ffdrnt6F1h0dDTPP/8848aNIyAggObNmzN//nzX86d3gX3//feYTCZWrVpFjx498PPzo1+/fm7hDODZZ58lLCyMgIAAJkyYwKOPPkrXrl3Ped+LFi1i2LBh3HXXXSxfvpy8PPf/48jKyuLOO+8kPDwcHx8fOnbsyGeffeZ6fu3atVxxxRX4+fkREhLC4MGDOXbsmOte586d63a9rl278vTTT7u+N5lMvPHGG4wYMQJ/f3+effZZrFYr48ePJyYmBl9fX9q2bcu///3vMrXHx8fToUMHvL29iYiIYPLkyQCMGzeOYcOGuZ1bUlJCkyZNiI+PP+fPRESkMuUVlrDlYBaf/HqQl7/6nUlLNnL17P8R++SXXDXrf9y5ZCMvfZnER78cZNOBLHILSvAwQVGJ1dC61QJUCU4UW2n/5FeGvPf2GYPx86qcX+MjjzzCrFmzWLRoEd7e3hQUFNC9e3ceeeQRAgMD+fzzzxkzZgwtW7akd+/eZ7zOrFmz+Oc//8ljjz3Gf/7zH+666y4GDBhAu3btzviaxx9/nFmzZtG4cWMmTZrEuHHjWLt2LQDvvvsuzz33HPPmzaN///4sW7aMWbNmERMTc9b7sdvtLFq0iNdff5127drRpk0bPvjgA/72t78BYLPZGDJkCLm5uSxdupRWrVqxfft216J/mzZt4qqrrmLcuHG88soreHp68t1332G1Vuw/2qeeeoqZM2cyZ84czGYzNpuNqKgoPvjgA0JDQ1m3bh133nknERER3HzzzQDExcUxdepUXnjhBYYMGUJ2drbr5zFhwgQGDBhAWlqaa5XmlStXcvz4cdfrRUQqk91u53BuYWlLznH2HM5jd4ajNSctu+CMr/O1mGkV5k+rxg1o3bgBrcIa0KpxA6JD/fD2NHaBVQUgcZkyZQp/+ctf3I499NBDrq/vvfdevvzySz788MOzBqDrrruOu+++G3CEqjlz5vD999+fNQA999xzXHHFFQA8+uijDB06lIKCAnx8fHj11VcZP368K7g8+eSTfP311xw/fvys9/PNN9+Qn5/P4MGDAbjttttYuHCh6zrffPMNiYmJ7NixgzZt2gDQsmVL1+tfeuklevTowbx581zHOnTocNb3LM9f//pXxo0b53bsmWeecX0dExPDunXr+OCDD1wB5tlnn+XBBx/k/vvvd53Xs2dPAPr160fbtm1ZsmQJDz/8MOBo6brpppto0KBBhesTEXEqsdpIPppfGm4cXVbOoJNbUHLG14U28KZVY39ahZ0MOq3DGhAR6IOHR81cU0wBqBL4WsxsnzHYsPeuLD169HD73mq18sILL7B8+XJSUlIoLCyksLAQf3//s16nc+fOrq+dXW3OfaXO5zXOVo2MjAyaN29OUlKSK1A59erVi2+//fas11y4cCGjRo3C09Px1/yWW27hH//4B0lJSbRt25ZNmzYRFRXlCj+n27RpEzfddNNZ3+N8nP5zBXjjjTd466232L9/PydOnKCoqMjVpZeRkUFqaipXXXXVGa85YcIE5s+fz8MPP0xGRgaff/45q1atuuhaRaR+yCssYe/hPHYfzmVPxsnWnH1H8ii2lj+0wsMEzRv6OVpzSltyHC06/gT7eVXzHVw8BaBKYDKZKq0bykinB5tZs2YxZ84c5s6dS6dOnfD392fKlCkUFRWd9TqnD/I1mUzYbLbzfo1zBeJTX3P6qsTnGvt09OhRPv30U4qLi4mLi3Mdt1qtxMfH8+KLL5YZ+H26cz3v4eFRpo7yto04/ef6wQcf8MADDzBr1iz69u1LQEAAL7/8Mj/99NN5vS/A7bffzqOPPsr69etZv3490dHRXH755ed8nYjUH3a7ncPHC0+25pSGnN0Z5+62atnY3xVynH+2aOSHTyX+T7fRav+ntlSZ1atXM2LECG677TbAEUh27dpFbGxstdbRtm1bEhMTGTNmjOvYhg0bzvqad999l6ioKD799FO346tWrWLmzJk899xzdO7cmYMHD7Jz585yW4E6d+7MqlWr3LqrTtW4cWO32XU5OTn88ccf57yf1atX069fP7dWrT179ri+DggIIDo6mlWrVjFw4MByr9GoUSNGjhzJokWLWL9+vatbT0TqH2e31eldVrszztVt5XVKK44z6PgTGeRbY7utKpMCkJxR69at+eijj1i3bh0hISHMnj2b9PT0ag9A9957LxMnTqRHjx7069eP5cuXs2XLFrfxOqdbuHAhN954Y5n1hlq0aMEjjzzC559/zogRIxgwYAA33HADs2fPpnXr1vz++++YTCauvfZapk2bRqdOnbj77ruZNGkSXl5efPfdd9x0002Ehobypz/9ibfffpvhw4cTEhLC9OnTz2vX9NatW/POO+/w1VdfERMTw5IlS/j555/dBnU//fTTTJo0ibCwMNdA7bVr13Lvvfe6zpkwYQLDhg3DarVyxx13XMBPVkRqixKrjSN5RaRnF/BHZp5byDlXt1Wzhn4nx+U0buAalFwbu60qkwKQnNH06dP5448/GDx4MH5+ftx5552MHDmS7Ozsaq3j1ltvZe/evTz00EMUFBRw8803M3bsWBITE8s9f+PGjWzevJkFCxaUeS4gIIBBgwaxcOFCRowYwUcffcRDDz3ELbfcQl5eHq1bt+aFF14AoE2bNnz99dc89thj9OrVC19fX3r37s0tt9wCwLRp09i7dy/Dhg0jKCiIf/7zn+fVAjRp0iQ2bdrEqFGjMJlM3HLLLdx999188cUXrnPuuOMOCgoKmDNnDg899BChoaHceOONbte5+uqriYiIoEOHDkRGRp73z1NEao6iEhuHjxdyKKeAjJxCDucWkJFbSEZOIRmlXx/KKeRoXiG2s/T8+1g8HK05p3RZtQ6re91WlclkP9+FZOqRnJwcgoKCyM7OJjAw0O25goIC/vjjD2JiYvDx8TGoQrnmmmto0qQJS5YsMboUw+Tn5xMZGUl8fHyZ2XuVQX/XRS7ciSKrK8A4w8yh0j8Pn3LsWH7ZcYNn4mFyzLaKDj19fE796bY6l7N9fp9OLUBS4+Xn5/PGG28wePBgzGYz77//Pt988w0JCQlGl2YIm81Geno6s2bNIigoiOuvv97okkTqBbvdzvHCknKDjDPoHMot4HBOIbmFZx57czqL2URYgA+NA7wJC/AmLNCb8AAfwgK9Tx4P9KaRvzdmhZxKowAkNZ7JZGLlypU8++yzFBYW0rZtWz766COuvvpqo0szRHJyMjExMURFRfH222+7pvmLyIWx2+1k5Rc7QkxugSvIOLqk3APOieLzXwjVx+JBWIAP4acFmbAAH7egE+xnKTPTVaqe/uWUGs/X15dvvvnG6DJqjOjo6PPeAkWkPrPb7RzNK+KQK9AUlLbYnOySOpzreBRZz75Ux6kCvD1pHFjaWnNqmAl0tuI4Wm8CvD0VbGowBSAREal18gpLOJRTQHppqEnPKeCQ61FIenZBhYNNsJ/lZKg5raXm1K/rwrpvogAkIiI1SLHVxuHcQleYSc8u4FBuIYeyCziU6/g+o4JjbBr5exEW6OyKOjXgeBMW6Ag2jQO8Dd+bSqqXApCIiFQ5u93OsfziU1ptCkjPdnRNnQw3hRzJK+R8e3j9vcyEB/kQHuBDkyBHqGkS6EO46+EIO16eHlV7c1IrKQCJiMhFyS8qcXU7ZZS20hzKOaUVp7Sb6ny7ozw9TIQFeJ8z3AT4WM59MZEzUAASEZFyFRRbOZZfdMZA4xx3c7btFk7X0N/LFWCaBPoQFuhTGmy8XeGmkb+X1rSRKqcAJBVy5ZVX0rVrV+bOnQs4ZiRNmTKFKVOmnPE1JpOJTz75hJEjR17Ue1fWdUTqE7vdTl6Rlaz8IrLyi8k+UUxWfjFZJ079vqj0WDHZpc9lnyimoPj8BxD7Wsw0CToZbMJPeTQJOjnuRuNspKZQAKonhg8fzokTJ8qdTr5+/Xr69evHxo0b6datW4Wu+/PPP5fZ7fxiPf3003z66ads2rTJ7XhaWhohISGV+l5ncuLECSIjIzGZTKSkpJzXDu0iVclms5NbUOIKLlmlwcUVaJzBpZznSs62h8I5mJ3dUeW22pSGm0AfTfmW8hXlQ24a5KSW/bNhDFwzw7DSFIDqifHjx/OXv/yF/fv306JFC7fn4uPj6dq1a4XDDzh2RK8uTZo0qbb3+uijj+jYsSN2u52PP/6YW2+9tdre+3R2ux2r1aoFD+uIYqvNFUyynWHG1fpS5PjzRPnHLmb5Jy+zB8F+FsfD14sgPwvBvhaCfB3Hgvy8CPY9+bzjmIUGXp7qjpKybDbIzywn2KRBburJPwvOsndkRNdqK7c8+he1nhg2bBhhYWG8/fbbPPXUU67j+fn5LF++nOeff54jR44wefJkVq9ezdGjR2nVqhWPPfaYa/PP8pzeBbZr1y7Gjx9PYmIiLVu25N///neZ1zzyyCN88sknHDx4kCZNmnDrrbfy5JNPYrFYePvtt3nmmWcAXP83uWjRIsaOHVumC2zr1q3cf//9rF+/Hj8/P9eu7g0aNABg7NixZGVlcdlllzFr1iyKiooYPXo0c+fOxWI5++DJhQsXctttt2G321m4cGGZAPTbb7/x8MMPs3r1aux2O127duXtt9+mVatWgCNUzpo1i927d9OwYUNuuOEGXnvtNfbt20dMTAy//vorXbt2BSArK4uQkBC+++47rrzySr7//nsGDhzIl19+yeOPP86WLVv46quvaN68OVOnTuXHH38kLy+P2NhYZs6c6bYidmFhIdOnT+f9998nIyOD5s2b8+ijjzJu3DguueQSJk2axEMPPeQ6f9u2bXTu3Jldu3a5apcLl1dYwva0HLYezGbP4eNuXU3O7qbjFZi+XR4/L7MjuJwaWPwsBJWGFucx1/elgcbH4qEWGjk/Z2u1cYac4+lgO8+/yxZ/CIyAgAgIjDz5Z8OWVXsf56AAVBnsdijON+a9LX5wHv+oeXp6cvvtt/P222/z5JNPuv4h/PDDDykqKuLWW28lPz+f7t2788gjjxAYGMjnn3/OmDFjaNmyJb179z7ne9hsNv7yl78QGhrKjz/+SE5OTrljgwICAnj77beJjIxk69atTJw4kYCAAB5++GFGjRrFtm3b+PLLL13ddUFBQWWukZ+fz7XXXkufPn34+eefycjIYMKECUyePJm3337bdd53331HREQE3333Hbt372bUqFF07dqViRMnnvE+9uzZw/r16/n444+x2+1MmTKFvXv30rKl4z/WlJQUBgwYwJVXXsm3335LYGAga9eupaTE8Y9BXFwcU6dO5YUXXmDIkCFkZ2ezdu3ac/78Tvfwww/zr3/9i5YtWxIcHMzBgwe57rrrePbZZ/Hx8WHx4sUMHz6cpKQkmjdvDsDtt9/O+vXreeWVV+jSpQt//PEHmZmZmEwmxo0bx6JFi9wCUHx8PJdffrnCzwU4NexsS8lma4oj9Jxvb1OgjyfBfqUtLb4Wx9eu8HLKMWdLTelxjaGRC1YZrTZuTNAg7LRgEwEBke5/egee1+dUdVMAqgzF+fB8pDHv/VgqeJ3fGJxx48bx8ssvu1oYANdO4iEhIYSEhLh9ON577718+eWXfPjhh+cVgL755ht27NjBvn37iIqKAuD5559nyJAhbuc98cQTrq+jo6N58MEHWb58OQ8//DC+vr40aNAAT0/Ps3Z5vfvuu5w4cYJ33nnHNQbptddeY/jw4bz44ouEh4cDEBISwmuvvYbZbKZdu3YMHTqUVatWnTUAxcfHM2TIENd4o2uvvZb4+HieffZZAF5//XWCgoJYtmyZqyWpTZs2rtc/++yzPPjgg9x///2uYz179jznz+90M2bM4JprrnF936hRI7p06eL2Pp988gkrVqxg8uTJ7Ny5kw8++ICEhARXq5AztAH87W9/48knnyQxMZFevXpRXFzM0qVLefnllytcW32TX1TCb6nnF3bCA73p1DSI2IhAGvl7Eex3srvJGXICfS3a1FIqV3W12rj+bAINwsFce5ciUACqR9q1a0e/fv2Ij49n4MCB7Nmzh9WrV/P1118DYLVaeeGFF1i+fDkpKSkUFhZSWFh43oOcd+zYQfPmzV3hB6Bv375lzvvPf/7D3Llz2b17N8ePH6ekpITAwMAK3cuOHTvo0qWLW239+/fHZrORlJTkCkAdOnTAbD75f8wRERFs3br1jNe1Wq0sXrzYrevutttu44EHHuCZZ57BbDazadMmLr/88nK70TIyMkhNTeWqq66q0P2Up0ePHm7f5+Xl8cwzz/DZZ5+RmppKSUkJJ06cIDk5GYBNmzZhNpu54ooryr1eREQEQ4cOJT4+nl69evHZZ59RUFDATTfddNG11iX5RSVsT81hSwXCTqemwXSKCqRj0yDCAnyqv2ipu4oLIC8DjmdAbjocP1SvW20qk+EBaN68ebz88sukpaXRoUMH5s6dy+WXX37G819//XXXWIrmzZvz+OOPc/vtt7ueLy4uZubMmSxevJiUlBTatm3Liy++yLXXXlt1N2Hxc7TEGMHiV6HTx48fz+TJk3n99ddZtGgRLVq0cH1Yz5o1izlz5jB37lw6deqEv78/U6ZMoaio6LyuXd4GnaePOfjxxx8ZPXo0zzzzDIMHD3a1pMyaNatC92G32884nuHU46eHFJPJhM125qm9X331FSkpKYwaNcrtuNVq5euvv2bIkCFnnRF2rtliHh4ervqdiouLyz339OD5j3/8g6+++op//etftG7dGl9fX2688UbX7+d8ZqpNmDCBMWPGMGfOHBYtWsSoUaPw86vY36G6xBl2tqZks/Xg+YWdjk2D6BwVpLAjF85mgxPHHGHmeLoj3Bw/BLmHSo8dKj2WXoFgg+Pz4FzBppa32lQmQwPQ8uXLmTJlCvPmzaN///68+eabDBkyhO3bt7vGNJwqLi6OadOmsWDBAnr27EliYiITJ04kJCSE4cOHA47ulaVLl7JgwQLatWvHV199xZ///GfWrVvHpZdeWjU3YjKddzeU0W6++Wbuv/9+3nvvPRYvXszEiRNdgWH16tWMGDGC2267DXCM6dm1axexsbHnde327duTnJxMamoqkZGOLsH169e7nbN27VpatGjB448/7jq2f/9+t3O8vLywWq3nfK/FixeTl5fnCgpr167Fw8PDrTuqohYuXMjo0aPd6gN44YUXWLhwIUOGDKFz584sXryY4uLiMgErICCA6OhoVq1a5epmPJVz1lxaWprr7+Pp0/3PZPXq1YwdO5Y///nPABw/fpx9+/a5nu/UqRM2m43//e9/bgOjT3Xdddfh7+9PXFwcX3zxBT/88MN5vXddcKFhp1PpIyzwPMNO7iFY/xqk/AJR3aH1NdCsN3h6Ve4NSc1TfKKcIHNqoCl9Li/j/LuiAMxejuDSIMzxZz1utalMhgag2bNnM378eCZMmADA3Llz+eqrr4iLi2PmzJllzl+yZAl///vfXf933rJlS3788UdefPFFVwBasmQJjz/+ONdddx0Ad911F1999RWzZs1i6dKl5dbh7OpxysnJqdT7rEkaNGjAqFGjeOyxx8jOzmbs2LGu51q3bs1HH33EunXrCAkJYfbs2aSnp593ALr66qtp27Ytt99+O7NmzSInJ6dMkGjdujXJycksW7aMnj178vnnn/PJJ5+4nRMdHc0ff/zBpk2biIqKIiAgAG9vb7dzbr31Vp566inuuOMOnn76aQ4fPsy9997LmDFjXN1fFXX48GH++9//smLFCjp27Oj23B133MHQoUM5fPgwkydP5tVXX2X06NFMmzaNoKAgfvzxR3r16kXbtm15+umnmTRpEmFhYQwZMoTc3FzWrl3Lvffei6+vL3369OGFF14gOjqazMxMtzFRZ9O6dWs+/vhjhg8fjslkYvr06W6tWdHR0dxxxx2MGzfONQh6//79ZGRkcPPNNwNgNpsZO3Ys06ZNo3Xr1uV2UdYFbmGnNPBUSdg5VdYBWPcKbFwM1tJ/T/avgbX/Bq8AaHkFtL4aLrkGgqLOfi2pOWw2yD9S2lJz6JSuqAz3lprjGVBYwc8O34alY2nCTgk4TU4LO+HgE6xgUwUMC0BFRUVs3LiRRx991O34oEGDWLduXbmvKSwsxMfH/R8mX19fEhMTXf83fqZz1qxZc8ZaZs6c6Zp6XR+MHz+ehQsXMmjQILeWtunTp/PHH38wePBg/Pz8uPPOOxk5ciTZ2efXBOvh4cEnn3zC+PHj6dWrF9HR0bzyyitu3Y8jRozggQceYPLkyRQWFjJ06FCmT5/O008/7Trnhhtu4OOPP2bgwIFkZWW5psGfys/Pj6+++or777+fnj17uk2Dv1DOAdXljd8ZOHAgAQEBLFmyhKlTp/Ltt9/yj3/8gyuuuAKz2UzXrl3p378/4AhLBQUFzJkzh4ceeojQ0FBuvPFG17Xi4+MZN24cPXr0oG3btrz00ksMGjTonPXNmTOHcePG0a9fP0JDQ3nkkUfKhPW4uDgee+wx7r77bo4cOULz5s157LHH3M4ZP348zz//POPGjbuQH1ONU5GwExbg7eq+uqiwc6qje2HNHNj0PthKuzOjekHnmyFlI+z+BvIOw++fOR4AjdudDEPN+4Kn95mvL1WjKO8MQea0kJN3GOxnb5F24+lTGmBKQ0zAaYHG+fBvrFZBg5ns5Q3cqAapqak0bdqUtWvX0q9fP9fx559/nsWLF5OUlFTmNY899hiLFi3is88+o1u3bmzcuJGhQ4e6Bp5GRETw17/+lc2bN/Ppp5/SqlUrVq1axYgRI7BarW6tPKcqrwWoWbNmZGdnlxmcW1BQwB9//EFMTEyZoCVSG6xdu5Yrr7ySgwcPnrW1rCb+XT897GxLyWZ3RjWGnVNl/A5rZsPWD8Fe2hIXMwAG/AOiLz/5f+w2G6Rvhl3fOMLQwcST54Njtk3LK6D1VY7uspAWZd9LLkxRPqT+Cgd/hrTNjsHDzoBTdLwCFzKBf+hprTRh5YccdUMZKicnh6CgoHI/v09n+CDo0weynm1w6/Tp00lPT6dPnz7Y7XbCw8MZO3YsL730kmumz7///W8mTpxIu3btMJlMtGrVir/97W8sWrTojDV4e3uX6WIRqWsKCws5cOAA06dP5+abb77grsLqUtGw06lpEJ2iqijsnCptC6z+F2xfAZQWc8kguPwhaF7OchEeHhB5qeNxxT8cg1/3fOcIQ7u/cXwgJ610PABC2ziC0CVXQ/N+YKkZ4bPGs9vh2B9wcAMcSHQEzfRtZ2+9sfiVdjOdoxvKvzGYDf+4lEpm2G80NDQUs9lMenq62/GMjIwz/sPs6+tLfHw8b775JocOHSIiIoL58+cTEBBAaGgo4Bhk+umnn1JQUMCRI0eIjIzk0UcfJSYmpsrvSaQme//99xk/fjxdu3ZlyZIlRpdTxoGj+azZncnP+46eV9hxzsaq0rDjVuDPjuCz88uTx2KHO4JPZNfzv45vCHT8i+Nht0P6Vtid4GghOvATZO50PH583fEBHX25o6us9dWOvZPEofB4aetOoiP0HPzZ0V11uoAIiOoJTbs7WtdO7YbyblD9dUuNYVgA8vLyonv37iQkJLhmtQAkJCQwYsSIs77WYrG41ppZtmwZw4YNc00vdvLx8aFp06YUFxfz0UcfuQaBitRXY8eOLTOWykhH84pYtyeTtbszWbM7kwNHT5Q559Sw42zhCa+OsONkt8P+tfDDy7D3e8cxkwd0vAEumwrh7S/u+iYTRHR2PC5/EE5kOd7H2TqUmwa7vnI8ABq2Kg1D10B0f7DUk0167XbHWKsDiY6gczARDv3m3pUIjtlSEV0cgSeqJzTrBYFN1SUl5TK0TW/q1KmMGTOGHj160LdvX+bPn09ycjKTJk0CYNq0aaSkpPDOO+8AsHPnThITE+nduzfHjh1j9uzZbNu2jcWLF7uu+dNPP5GSkkLXrl1JSUnh6aefxmaz8fDDDxtyjyLicKLISuK+o6wrDTy/pboP4Pb0MHFp82D6tmxE56jg6g87p7LbYc8q+OFfkFy6lIOHJ3QZ7Qg+japo6xDfYOgw0vGw2x0f8rsTYPcqRx1H98BPe+CnNxyDbaMvPzmYuqpqMkJhrmMA+cGfHS1vB3+GE0fLnhcYBVE9HEEnqpcjSGpAuZwnQwPQqFGjOHLkCDNmzCAtLY2OHTuycuVK127laWlprlVuwbEY3axZs0hKSsJisTBw4EDWrVtHdHS065yCggKeeOIJ9u7dS4MGDbjuuutYsmQJwcHBlVq7QWPHRarNxf4dL7Ha2JqS7Wrh+WV/FkVW9/9jb9ckgP6tQ+nfuhG9YhrRwNvgcRY2G+z8wtHik/qr45jZC7rdDv3vh+Cy65NVGZMJmnR0PC57AApy4I//wa4ER+tQTkppOEqALx+BkJiTXWXRl4NXLVng0maDI7tPtuwc3AAZ28tp3fF2dDW6te4YtAWR1AmGzQKryc42itxqtbJz507CwsJo1KiRQRWKVL0jR46QkZFBmzZt3LYTORO73c6ew3ms3e3o1lq/9wi5Be6LvUUG+dC/dSiXXRJK31aNas5KyjYrbP8UfpgFGb85jln8oMc46DvZschcTWK3w+HfS8NQAuxff3IKPjjCQnT/0sHU10Cj1jWnG6gg29G642zZOfgzFGSVPS+oOTQrDTtRvaBJJ00bl3OqyCwwBaBynOsHmJaWRlZWFmFhYfj5+Z1x1ppIbWS328nPzycjI4Pg4GAiIs784Z+RU8DaPZms2XWEtbszSc8pcHs+0MeTfq1C6X9JKJe1DiW6UQ3778Va7JjGvnqWoxUCHIsW9r4T+tztmPpcGxTmwh8/OFqGdn0D2cnuzwe3ONlVFjOg+laut9kcA7oPJp7szjr8O67Zc06ePhDZ7ZTurJ6OmVkiFaQAdJHO9QO02+2kp6eTlZVV/cWJVJPg4GCaNGniFlhyC4r5ae9R1pS28uzKcF9LxcvTg57RIfRr5Qg8HZsG1cxdz0sKYdO7jgUMs0rDgm+II/T0muj4uray2x2hw9U6tA6sp+znZ/ZyLL7oHEzduG3ltQ6dOAYHN57SnbURCstZSDUk+mTLTlQPR+uO9qeSSqAAdJHO9wdotVrPuJGlSG1msVgwm80Uldj4NfkYa/c4Wng2HcjCesrcdJMJOkYGObq1WofSIzoEH8u5u8sMU5QPvyx2bE+Rm+Y45t8Y+t3r6O7yDjC2vqpQlAd/rC6dap8AWe577xHUzNE61Ppqx4KM5/szsFnhcJIj6Di7szLLLmCLxc/RuuPqzurpWF9HpAooAF2kivwAReoSm81O0qFc18DlxD+Okl/kvpBcdCM/V+Dp26oRwX61YFxGQQ5sWAjrXoP8TMexgEi4bApcOqb2DBi+WHa7o6tv9zeOMLRvzcl9ywA8LNC8z8nWobDYk61D+UdL19txTkXfCEW5Zd+jYcuTLTvNekFYBy0iKNVGAegiKQBJfXLwWH7pwOUjrNuTSebxIrfnG/l70a91KJe1bkS/VqE0a1iLwsKJY/DTm/Bj3MmBtsEt4PKp0OUWTZkuyneEoN3fOFqIju51fz6wqWMF68O/nxwjdSqvBtC0m3t3Vm0ZNyV1Uq3aCkNEqldWfhHr9xxxjePZdyTf7Xlfi5neLRtyWetQ+rcOpW14AB41cRzP2Rw/7FhJOfGtk60UjS6BAQ9BxxvVIuHk5QdtBjkeAEf2nNI6tNox1T4n5eT5jS4pnYJe2pUV1h48anCXp8hZ6F8BkTquoNjKhn3HWLM7k3V7Mtmaks2p7b5mDxNdmwU71uNp1YhLm4fg5elx5gvWZDmpsO5V2LAISkpXlg7v6Ag+sdfrw/pcGrVyPHr/HYpPOFbBPvQbNI51tO74NTS6QpFKowAkUsdYbXZ+S812tfD8vO8YRSXui8pdEtbANY6nd8uGBPjU8hk4x/bD2rnw69KTM54iu8EVD0Oba2vOGji1icX35OBokTpIAUikDjh4LJ/vkg6zdpdjAcLsE+6zE5sEOhcgdIzjMWyLicqWuRvWzIYty8FWuuhi836OXddbDlTwEZEzUgASqaUycgv4fEsa/92cyi/JWW7PBfh40rdlIy67JJR+rUJp1di/Zi1AeLEO/eZYvPC3T05umdDqT46d2aP7G1ubiNQKCkAitUhWfhFfbktnxeZUftx7BOeSPCYT9IxuyIBLHAOXOzUNwtNcS8fxnE3KL47g8/tnJ4+1GQID/gFR3Y2rS0RqHQUgkRour7CEb3YcYsWmVH7YdZhi68kRzJc2D+b6LpEM7RRBWF3p1ipP8o+ODUp3f1N6wOTYMf3yBx2rCIuIVJACkEgNVFBs5fukw/x3SyqrdhyioPjkIObYiECGd4lgeOfI2rUmT0XZ7Y7dz3/4l2NKNoDJDJ1ucgSfxm2MrU9EajUFIJEaothqY92eI6zYlMrXv6WTW3hyJ/XoRn5c3yWS4V0iuST8lK0KbDZI3+JoGcnaDx6ejr2enH+aLY7Vfc3Oh/M5SznnnfK12+vOcp6HZ+UPNLbbYdfXjhafgz87jnlY4NJbof8UaBhTue8nIvWSApCIgWw2Oz/vO8p/t6Sycms6R/NOrsIcEeTD8C6RDO8cScemgScHMecfhT3flq7euwryMgyqvtSpAcujNDCZPd2/NnuVc145rzF7Obq70rc4ru3pA93ugP73QVCUsfcpInWKApBINbPb7WxNyWbFplQ+25JGek6B67lG/l5c1ymC67tG0r15iGMFZpsNUn+BXaXbFaRsPDnzCRzbEcRcAZFdHcetRWAtdjxsxaXfl7h/bS0q/f7Ur099jfNR5Jhe7vzabi17Q7bS11TmvsAWf+g1AfpO1saZIlIlFIBEqsnOQ7n8d3Mq/92c6rb9RICPJ9d2aMLwLpH0a9XIMXsr7whs+/BkK49zA0+nsPbQ+irHhpXN+4JnNW1IarOdDEhnC0q2knKCWDnPlXeeTzB0/atWHRaRKqUAJFKFko/k898tjtDze/rJnbN9LB5cHRvO9V0iuaJtY7w9gNRf4Ye3Slt5fgFO2a/CKwBaXlG6S/fVxnUHeXiAh7c2ERWRWk8BSKSSHcop4LMtaazYnMrmA1mu4xaziSvahDG8SwRXx4bjX3wM9qyCTxMcY3pOHHW/UHhHR9i55BrHTtvV1cojIlIPKACJVIJjeUV8sS2dFZtT+OmPo67NRj1M0K9VKNd3iWRwbGOCjm6G3Ytg8TeOFp9TeQdBqysd3Vqtr4LAyGq/DxGR+kIBSOQC5RYUk7D9EP/dnMrqXZmU2E52WXVvEeJYoLClB6Hpa2DXG7DqWyjIcr9Ik86ntPL0dMyKEhGRKqcAJFIBBcVWvvs9gxWbU/n29wwKT9llvUNkINd3CuPPjVMJO/QNbEmAr7e4X8AnyLFnlbOVJ6BJNd+BiIiAApDIORVbbazZlcl/N6fy9fZDHD9lgcKWjf25pZ2FEQ12EHboA/jpOyjIdr9ARNfSwcvXQNPujvVuRETEUPqXWKQcVpudxD+OsmJzKl9sSyMr/+QiN82DLNzZ8jCDvbcRmvYDpp+3ub/YNwRaXeUIPa3+pHVsRERqIAUgkVJ2u51NB7L47+Y0PtuSSkZuoeu59v7HmdR0L5ebNhGcvg7TjpxTXmmCpt0cY3laX+P42sNc/TcgIiLnTQFI6jW73U7SoVxWbErlv1tSOXD0BAAWSrjKZzdjQnfRs2Qj/lk7IfmUF/o1cm/l8Q815gZEROSCKABJvbVuTyYvfPE7Ww46xuxEkskdXlu5IXAH7Qt+xbMkD1wLMJsgqoejheeSqyHiUseigCIiUispAEm9szvjOLM//5XdO3+jhekQ0y1JDPHZRmTxfscJx0tP9G9c2q11taOVR1sziIjUGQpAUjfZ7Y5d04/9AUf/gGN/UJCxm/R9Owg4nsw8UxacuptDMWDycKy43PpqRytPky5q5RERqaMUgKT2stkgJ8Ut5Lj9WZjjdroPEA1gcnxv9Q7C3DCmdMuJq6DVQMcMLhERqfMUgKRmKy6ArP3lB5ys/Y6dxM/ihE8YSYWh7CoOZb89HHtINNde3o9OnbpiVpeWiEi9pQAkxjuRVTbcOL/OScVtV/TTeVgguDk0jIGQGNefm/ND+OeaPDakOqayRwb58PC17bi+SyQeHqZquS0REam5FICk6tlscDy9/FacY3/AiWNnf71XADSMdgs4rj+DotzW3Nlz+DgzV/7ONzsOAdDA25O7B7ZiXP8YfCxam0dERBwUgKRylBRBVnL5AefYPigpOPvr/cPKhpuGMdCwpWPNHdPZW22OHC/k36t28e5PyVhtdsweJv7aqzn3X30JoQ28z/paERGpfxSApOLsdtj2EexbfTLkZB8Eu+3MrzGZIbhZ+a04IdHg3eCCSikotrJo7T7mfbeb3NI9uq6ODePRIbG0Druwa4qISN2nACQVY7PCyn/AhoVln7P4OcJMw5alf54SdIKagdlSeWXY7Px3SyovfZlESpZj9eaOTQN57LpY+rXSqswiInJ2CkBy/ooL4OOJsGMFYILef4eILidDToPwc3ZVVYbEP47y3Ofb2Vy6gnNEkA//GNyWkV2baoCziIicFwUgOT8F2bDsVke3l9kL/jIfOvy5WkvYe/g4L375O1/95hjg7O9l5u6BrRnXPwZfLw1wFhGR86cAJOeWmw5Lb4RDWx0zsm55D2IGVNvbH80r4pVVu1j6435KSgc4j+7ZjClXt6FxgAY4i4hIxSkAydkd2QNL/uxYdNA/DG77j6PbqxoUFFtZvG4fr323m9wCxwDnq9qF8eiQdlwSHlAtNYiISN2kACRnlvILvHsT5Gc6xvmM+cQx1qeK2e12/rsljZe+/J2DxxwDnNtHBPL40Fj6t9YAZxERuXiG7/Q4b948YmJi8PHxoXv37qxevfqs57/++uvExsbi6+tL27Zteeedd8qcM3fuXNq2bYuvry/NmjXjgQceoKDgHOvQiLs938LbwxzhJ6ILjP+6WsLPz/uOMnLeOu57/1cOHjtBk0Af/nVTFz679zKFHxERqTSGtgAtX76cKVOmMG/ePPr378+bb77JkCFD2L59O82bNy9zflxcHNOmTWPBggX07NmTxMREJk6cSEhICMOHDwfg3Xff5dFHHyU+Pp5+/fqxc+dOxo4dC8CcOXOq8/Zqr63/gU8mga0YYq6A0e+Cd9V2Oe3LzOOFL37ny9/SAfDzMnPXFa2YcHlLDXAWEZFKZ7Lb7WfZaKlq9e7dm27duhEXF+c6Fhsby8iRI5k5c2aZ8/v160f//v15+eWXXcemTJnChg0bWLNmDQCTJ09mx44drFq1ynXOgw8+SGJi4jlbl5xycnIICgoiOzubwMDAC7292unHOPjyUcfXHf4Cf34DPKtuoPGxvCJe+dYxwLnYasfDBKN6NueBay4hLMCnyt5XRETqnop8fhvWAlRUVMTGjRt59NFH3Y4PGjSIdevWlfuawsJCfHzcPxR9fX1JTEykuLgYi8XCZZddxtKlS0lMTKRXr17s3buXlStXcscdd5yxlsLCQgoLC13f5+TkXMSd1VJ2O6x6BtaUtpL1ngSDZ4JH1fSSFpZYeWfdfl79dhc5pQOcr2zbmMeui6WNBjiLiEgVMywAZWZmYrVaCQ8PdzseHh5Oenp6ua8ZPHgwb731FiNHjqRbt25s3LiR+Ph4iouLyczMJCIigtGjR3P48GEuu+wy7HY7JSUl3HXXXWWC1qlmzpzJM888U6n3V6tYS+C/98Gmdx3fX/UUXPZAlSxqaLfb+XxrGi9++TsHjjoGOLdrEsDjQ2O5/JLGlf5+IiIi5TF8FpjptA9Zu91e5pjT9OnTSU9Pp0+fPtjtdsLDwxk7diwvvfQSZrNjnMj333/Pc889x7x58+jduze7d+/m/vvvJyIigunTp5d73WnTpjF16lTX9zk5OTRr1qyS7rCGK8qH//wNdn7p2K9r+L+h25gqeauN+4/y7Oc7+DU5C4DwQG8eHNSWG7pFYdYKziIiUo0MC0ChoaGYzeYyrT0ZGRllWoWcfH19iY+P58033+TQoUNEREQwf/58AgICCA11zBCaPn06Y8aMYcKECQB06tSJvLw87rzzTh5//HE8yunS8fb2xtu7Hi6ol38U3hsFBxPB0wduehvaDqn0t9l/JI8Xv/ydlVtPDnD++4BWTBwQg5+X4RlcRETqIcM+fby8vOjevTsJCQn8+c8nt1RISEhgxIgRZ32txWIhKioKgGXLljFs2DBXsMnPzy8TcsxmM3a7HQPHe9c82QdhyV8gMwl8guGvH0Dz3pX6Fln5Rbz67W7eWb/vlAHOzXjg6jaEBWqAs4iIGMfQ//2eOnUqY8aMoUePHvTt25f58+eTnJzMpEmTAEfXVEpKimutn507d5KYmEjv3r05duwYs2fPZtu2bSxevNh1zeHDhzN79mwuvfRSVxfY9OnTuf76613dZPVexg5YegPkpEBgU7jtIwiLrbTLF5ZYWbJ+P69+u5vsE8UAXNGmMdOua0e7JvVsVp2IiNRIhgagUaNGceTIEWbMmEFaWhodO3Zk5cqVtGjRAoC0tDSSk5Nd51utVmbNmkVSUhIWi4WBAweybt06oqOjXec88cQTmEwmnnjiCVJSUmjcuDHDhw/nueeeq+7bq5mSf4L3boaCLAhtC2M+hqCoSrm03W7ni23pvPDF7yQfzQccA5wfuy6WAW00wFlERGoOQ9cBqqnq7DpASV/Ah2OhpACiesFfl4Nfw0q59C/Jx3ju8x1s3H8MgMYB3jw0qA03dm+mAc4iIlItasU6QFLNfl0KK+4DuxUuGewY8Ozld9GXPXA0nxe+/J3Pt6QB4Gsx8/crWjLx8pb4e+uvl4iI1Ez6hKrr7HZYMxtWzXB83/VWx1R3s+WiLpudX8xr3+1i8br9FFltmExwc/dmTB3UhnANcBYRkRpOAagus9ngq2nw0xuO7y97wLHI4UUucHiiyMqI19ew74hjnM/ll4Ty2HWxxEbUoe5CERGp0xSA6qqSQvj0Ltj2keP7wTOh792VculNB7LYdySfIF8L/x7dlSvbhlXKdUVERKqLAlBdVJgLy2+Dvd+Dh8WxoWmnGyvt8ltTsgDo16qRwo+IiNRKCkB1zfHD8O4NkLYZLP4weim0+lOlvsWWg9kAdIoKqtTrioiIVBcFoLrk6B+w9C9wdC/4hcKtH0LTbpX+Ns4A1LlpcKVfW0REpDooANUVaZth6Y2QlwHBLWDMJ9CoVaW/TVZ+kWuRw05N1QIkIiK1kwJQXfDHD/D+X6EoF8I7wW3/gYAmVfJWW1McrT/RjfwI8ru4qfQiIiJGUQCq7X77BD6+E6xFEH05jH4XfKquZebk+J/gKnsPERGRqqYAVJslLoCV/wDs0H4E/Hk+WKp2EcKtrvE/6v4SEZHaSwGoNrLb4bvn4IeXHd/3GA/XvQweVb/bvbMLTDPARESkNlMAqm2sJfD5VPhlseP7gY/DgH9c9OrO5yPzeCEpWScwmaBDpFZ9FhGR2ksBqDYpPgH/GQ9Jn4PJA4bOhh5/q7a3d3Z/tQz1J8BHA6BFRKT2UgCqLU4cg/dvgeT1YPaGGxdC7PBqLcG1/o8GQIuISC2nAFQb5KTC0hsgYzt4B8Et70N0/2ovw7kFRmeN/xERkVpOAaimO7zTsbpz9gFo0ATGfAzhHQwp5WQLkAKQiIjUbgpANdnBDfDuTXDiKDRqDbd9DCEtDCnlUE4BGbmFeJigfYQCkIiI1G4KQDXVrgT44HYozoem3eGvH4B/qGHlOFt/2oQH4OtV9dPtRUREqpICUE206X1YMRlsJdDqKrj5HfBuYGhJWw9mAdr/S0RE6gYPowuQ06x9BT6d5Ag/nUfBX5cbHn4ANmv8j4iI1CFqAaopbDZImA7rX3N833cyXPNP8DA+o9rt9lNWgA42thgREZFKoABUE1iL4f/ugS3LHd9f80/of5+xNZ0iJesER/OKsJhNxEYEGF2OiIjIRVMAMlrhccdg5z2rwMMTRrwOXUYbXZUb5wrQbZsE4O2pAdAiIlL7KQAZKS/TMc099Rew+DkGO19yjdFVlbHF2f3VNNjYQkRERCqJApBRju13LHB4ZDf4NoRbP4SoHkZXVa6tGgAtIiJ1jAKQEdK3Oba2OJ4OQc0cCxw2bmN0VeWy2+1s0RR4ERGpYxSAqtu+tY5NTQuzIaw93PYRBEYaXdUZ7T+ST05BCV6eHrRtogHQIiJSNygAVaedX8Py28BaCM37wS3vgW+I0VWdlXP8T/uIQCxm46fki4iIVAYFoOrUqBX4BEJUL7hxIVh8ja7onJwrQGv8j4iI1CUKQNWpUSsYn+AY92OuHT965x5gGv8jIiJ1Se34FK5LGsYYXcF5s9nsbEtxzgALNrYYERGRSqRBHXJGezPzyCuy4msx06qxv9HliIiIVBoFIDmjrSlZAHSIDMRTA6BFRKQO0aeanNHmA84NUDX+R0RE6hYFIDkj5w7wXTT+R0RE6hgFIClXidXGb6lqARIRkbpJAUjKtfvwcQqKbTTw9iSmkQZAi4hI3aIAJOVyrv/TsWkgHh4mg6sRERGpXApAUq6TO8AHG1uIiIhIFVAAknJpB3gREanLFICkjKISGzvScgHtASYiInWT4QFo3rx5xMTE4OPjQ/fu3Vm9evVZz3/99deJjY3F19eXtm3b8s4777g9f+WVV2Iymco8hg4dWpW3UafsPJRLkdVGkK+F5g39jC5HRESk0hm6F9jy5cuZMmUK8+bNo3///rz55psMGTKE7du307x58zLnx8XFMW3aNBYsWEDPnj1JTExk4sSJhISEMHz4cAA+/vhjioqKXK85cuQIXbp04aabbqq2+6rttrjG/wRhMmkAtIiI1D2GtgDNnj2b8ePHM2HCBGJjY5k7dy7NmjUjLi6u3POXLFnC3//+d0aNGkXLli0ZPXo048eP58UXX3Sd07BhQ5o0aeJ6JCQk4OfnpwBUAc4tMDT+R0RE6irDAlBRUREbN25k0KBBbscHDRrEunXryn1NYWEhPj4+bsd8fX1JTEykuLi43NcsXLiQ0aNH4+9/5rVsCgsLycnJcXvUZ6e2AImIiNRFhgWgzMxMrFYr4eHhbsfDw8NJT08v9zWDBw/mrbfeYuPGjdjtdjZs2EB8fDzFxcVkZmaWOT8xMZFt27YxYcKEs9Yyc+ZMgoKCXI9mzZpd+I3VcgXFVpLSHQOgO2kKvIiI1FGGD4I+fYyJ3W4/47iT6dOnM2TIEPr06YPFYmHEiBGMHTsWALPZXOb8hQsX0rFjR3r16nXWGqZNm0Z2drbrceDAgQu7mTpgR1oOJTY7jfy9iAzyOfcLREREaiHDAlBoaChms7lMa09GRkaZViEnX19f4uPjyc/PZ9++fSQnJxMdHU1AQAChoaFu5+bn57Ns2bJztv4AeHt7ExgY6Paor5wboHbSAGgREanDDAtAXl5edO/enYSEBLfjCQkJ9OvX76yvtVgsREVFYTabWbZsGcOGDcPDw/1WPvjgAwoLC7ntttsqvfa6bItWgBYRkXrA0GnwU6dOZcyYMfTo0YO+ffsyf/58kpOTmTRpEuDomkpJSXGt9bNz504SExPp3bs3x44dY/bs2Wzbto3FixeXufbChQsZOXIkjRo1qtZ7qu1cW2BoBpiIiNRhhgagUaNGceTIEWbMmEFaWhodO3Zk5cqVtGjRAoC0tDSSk5Nd51utVmbNmkVSUhIWi4WBAweybt06oqOj3a67c+dO1qxZw9dff12dt1Pr5ReVsCvDOQBaAUhEROouk91utxtdRE2Tk5NDUFAQ2dnZ9Wo80IZ9R7nxjfWEB3rz02NXG12OiIhIhVTk87vCY4Cio6OZMWOGW8uM1A3O8T+dmgYbW4iIiEgVq3AAevDBB/m///s/WrZsyTXXXMOyZcsoLCysitqkmjl3gNcCiCIiUtdVOADde++9bNy4kY0bN9K+fXvuu+8+IiIimDx5Mr/88ktV1CjVZMspU+BFRETqsgueBt+lSxf+/e9/k5KSwlNPPcVbb71Fz5496dKlC/Hx8WhoUe2SW1DM3sN5gGaAiYhI3XfBs8CKi4v55JNPWLRoEQkJCfTp04fx48eTmprK448/zjfffMN7771XmbVKFdqW4tj/rGmwL40aeBtcjYiISNWqcAD65ZdfWLRoEe+//z5ms5kxY8YwZ84c2rVr5zpn0KBBDBgwoFILlarl3AFe439ERKQ+qHAA6tmzJ9dccw1xcXGMHDkSi8VS5pz27dszevToSilQqodrBpgCkIiI1AMVDkB79+51LVR4Jv7+/ixatOiCi5Lq59wDrLOmwIuISD1Q4UHQGRkZ/PTTT2WO//TTT2zYsKFSipLqlZVfxP4j+QB00gBoERGpByocgO655x4OHDhQ5nhKSgr33HNPpRQl1cvZ+tOikR9BfmW7NEVEROqaCgeg7du3061btzLHL730UrZv314pRUn10g7wIiJS31Q4AHl7e3Po0KEyx9PS0vD0NHRvVblA2gFeRETqmwoHoGuuuYZp06aRnZ3tOpaVlcVjjz3GNddcU6nFSfXYqhWgRUSknqlwk82sWbMYMGAALVq04NJLLwVg06ZNhIeHs2TJkkovUKpW5vFCUrJOYDJBh8iz75wrIiJSV1Q4ADVt2pQtW7bw7rvvsnnzZnx9ffnb3/7GLbfcUu6aQFKzObu/Wob6E+Cj35+IiNQPFzRox9/fnzvvvLOyaxEDaAC0iIjURxc8ann79u0kJydTVFTkdvz666+/6KKk+ji3wND6PyIiUp9c0ErQf/7zn9m6dSsmk8m167vJZALAarVWboVSpZwtQF2aKQCJiEj9UeFZYPfffz8xMTEcOnQIPz8/fvvtN3744Qd69OjB999/XwUlSlU5lFNARm4hHiZoH6EAJCIi9UeFW4DWr1/Pt99+S+PGjfHw8MDDw4PLLruMmTNnct999/Hrr79WRZ1SBZytP23CA/D1MhtcjYiISPWpcAuQ1WqlQYMGAISGhpKamgpAixYtSEpKqtzqpEptPZgFaPyPiIjUPxVuAerYsSNbtmyhZcuW9O7dm5deegkvLy/mz59Py5Ytq6JGqSKbXTPAFIBERKR+qXAAeuKJJ8jLywPg2WefZdiwYVx++eU0atSI5cuXV3qBUjXsdvspK0AHG1uMiIhINatwABo8eLDr65YtW7J9+3aOHj1KSEiIayaY1HwpWSc4mleEp4eJdk0CjC5HRESkWlVoDFBJSQmenp5s27bN7XjDhg0VfmoZ5wrQ7SIC8LFoALSIiNQvFQpAnp6etGjRQmv91AFbnN1fTYONLURERMQAFZ4F9sQTTzBt2jSOHj1aFfVINdmqAdAiIlKPVXgM0CuvvMLu3buJjIykRYsW+Pv7uz3/yy+/VFpxUjXsdjtbNAVeRETqsQoHoJEjR1ZBGVKd9h/JJ6egBC9PD9qEawC0iIjUPxUOQE899VRV1CHVyDn+JzYiEC/PCveCioiI1Hr69KuHnCtAd1b3l4iI1FMVbgHy8PA465R3zRCr+bZoALSIiNRzFQ5An3zyidv3xcXF/PrrryxevJhnnnmm0gqTqmGz2dmW4gxAwcYWIyIiYpAKB6ARI0aUOXbjjTfSoUMHli9fzvjx4yulMKkaezPzyCuy4msx06qx/7lfICIiUgdV2hig3r17880331TW5aSKbE3JAqBDZCCeZg0BExGR+qlSPgFPnDjBq6++SlRUVGVcTqrQ5gPODVA1/kdEROqvCneBnb7pqd1uJzc3Fz8/P5YuXVqpxUnl25qiAdAiIiIVDkBz5sxxC0AeHh40btyY3r17ExISUqnFSeUqsdr4LVV7gImIiFQ4AI0dO7YKypDqsPvwcQqKbTTw9qRlqAZAi4hI/VXhMUCLFi3iww8/LHP8ww8/ZPHixZVSlFQN5/o/HZsG4uFx5rWcRERE6roKB6AXXniB0NDQMsfDwsJ4/vnnK6UoqRond4APNrYQERERg1U4AO3fv5+YmJgyx1u0aEFycnKlFCVVw7kHmHaAFxGR+q7CASgsLIwtW7aUOb5582YaNWpU4QLmzZtHTEwMPj4+dO/endWrV5/1/Ndff53Y2Fh8fX1p27Yt77zzTplzsrKyuOeee4iIiMDHx4fY2FhWrlxZ4drqkqISGztScwDNABMREanwIOjRo0dz3333ERAQwIABAwD43//+x/3338/o0aMrdK3ly5czZcoU5s2bR//+/XnzzTcZMmQI27dvp3nz5mXOj4uLY9q0aSxYsICePXuSmJjIxIkTCQkJYfjw4QAUFRVxzTXXEBYWxn/+8x+ioqI4cOAAAQEBFb3VOmXnoVyKrDYCfTxp3tDP6HJEREQMZbLb7faKvKCoqIgxY8bw4Ycf4unpyE82m43bb7+dN954Ay8vr/O+Vu/evenWrRtxcXGuY7GxsYwcOZKZM2eWOb9fv37079+fl19+2XVsypQpbNiwgTVr1gDwxhtv8PLLL/P7779jsVgqcmsuOTk5BAUFkZ2dTWBg4AVdo6Z576dkHvtkK5e1DmXphN5GlyMiIlLpKvL5XeEuMC8vL5YvX05SUhLvvvsuH3/8MXv27CE+Pr5C4aeoqIiNGzcyaNAgt+ODBg1i3bp15b6msLAQHx8ft2O+vr4kJiZSXFwMwIoVK+jbty/33HMP4eHhdOzYkeeff/6su9QXFhaSk5Pj9qhrnFtgqPtLRETkArrAnC655BIuueSSC37jzMxMrFYr4eHhbsfDw8NJT08v9zWDBw/mrbfeYuTIkXTr1o2NGzcSHx9PcXExmZmZREREsHfvXr799ltuvfVWVq5cya5du7jnnnsoKSnhySefLPe6M2fOrPM72W85qBWgRUREnCrcAnTjjTfywgsvlDn+8ssvc9NNN1W4gFNXlQbH1hqnH3OaPn06Q4YMoU+fPlgsFkaMGOFamNFsNgOO7riwsDDmz59P9+7dGT16NI8//rhbN9vppk2bRnZ2tutx4MCBCt9HTVZQbCUpPReATpoCLyIiUvEA9L///Y+hQ4eWOX7ttdfyww8/nPd1QkNDMZvNZVp7MjIyyrQKOfn6+hIfH09+fj779u0jOTmZ6OhoAgICXGsTRURE0KZNG1cgAse4ovT0dIqKisq9rre3N4GBgW6PumRHWg4lNjuN/L2IDPI59wtERETquAoHoOPHj5c71sdisVRo7IyXlxfdu3cnISHB7XhCQgL9+vU762stFgtRUVGYzWaWLVvGsGHD8PBw3Er//v3ZvXs3NpvNdf7OnTuJiIio0BilusS5AWqnqKAztq6JiIjUJxUOQB07dmT58uVlji9btoz27dtX6FpTp07lrbfeIj4+nh07dvDAAw+QnJzMpEmTAEfX1O233+46f+fOnSxdupRdu3aRmJjI6NGj2bZtm9sK1HfddRdHjhzh/vvvZ+fOnXz++ec8//zz3HPPPRW91TrDNf5HCyCKiIgAFzAIevr06dxwww3s2bOHP/3pTwCsWrWK9957j//85z8VutaoUaM4cuQIM2bMIC0tjY4dO7Jy5UpatGgBQFpamtvq0larlVmzZpGUlITFYmHgwIGsW7eO6Oho1znNmjXj66+/5oEHHqBz5840bdqU+++/n0ceeaSit1pnOLfA0PgfERERhwqvAwS4WlU2bdqEr68vXbp04amnniIwMJCuXbtWQZnVqy6tA5RfVELHp77CZoefHruK8ECNARIRkbqpIp/fFzQNfujQoa6B0FlZWbz77rtMmTKFzZs3n3W9Hal+21NzsNkhPNBb4UdERKRUhccAOX377bfcdtttREZG8tprr3HdddexYcOGyqxNKoFz/E+npsHGFiIiIlKDVKgF6ODBg7z99tvEx8eTl5fHzTffTHFxMR999FGFB0BL9dhyMAvQAogiIiKnOu8WoOuuu4727duzfft2Xn31VVJTU3n11VersjapBFtOmQIvIiIiDufdAvT1119z3333cdddd13UFhhSfXILitl7OA+ATpoCLyIi4nLeLUCrV68mNzeXHj160Lt3b1577TUOHz5clbXJRdqW4liYsmmwL6ENvA2uRkREpOY47wDUt29fFixYQFpaGn//+99ZtmwZTZs2xWazkZCQQG5ublXWKRdAO8CLiIiUr8KzwPz8/Bg3bhxr1qxh69atPPjgg7zwwguEhYVx/fXXV0WNcoFcM8AUgERERNxc8DR4gLZt2/LSSy9x8OBB3n///cqqSSqJcw+wzpoCLyIi4uaiApCT2Wxm5MiRrFixojIuJ5UgK7+I/UfyAQ2AFhEROV2lBCCpeZytPy0a+RHkZzG4GhERkZpFAaiOOrkCtFp/RERETqcAVEc5d4DXDDAREZGyFIDqKNcA6KhgYwsRERGpgRSA6qDM44WkZJ3AZIIOkYFGlyMiIlLjKADVQc7Wn5ah/gT4aAC0iIjI6RSA6qAtB9T9JSIicjYKQHWQcwsMzQATEREpnwJQHbRFM8BERETOSgGojjmUU0BGbiEeJugQqQAkIiJSHgWgOsbZ+tMmPABfL7PB1YiIiNRMCkB1zNaDWYDG/4iIiJyNAlAds1njf0RERM5JAagOsdvtrjWAOmkKvIiIyBkpANUhKVknOJpXhKeHiXZNAowuR0REpMZSAKpDnBugtm0SgI9FA6BFRETORAGoDtmiDVBFRETOiwJQHbJVA6BFRETOiwJQHWG329miKfAiIiLnRQGojth/JJ+cghK8PD1oE64B0CIiImejAFRHOMf/xEYE4uWpX6uIiMjZ6JOyjnCuAN1Z3V8iIiLnpABURzj3AOukAdAiIiLnpABUB9hsdraVdoF10RR4ERGRc1IAqgP2ZuaRV2TF12KmVWN/o8sRERGp8RSA6oCtKVkAdIgMxNOsX6mIiMi56NOyDth8QON/REREKkIBqA7YmqIVoEVERCpCAaiWK7Ha+C21tAWoabCxxYiIiNQSCkC13O7DxykottHA25OWoRoALSIicj4UgGo55/o/HZsG4uFhMrgaERGR2kEBqJY7uQN8sLGFiIiI1CKGB6B58+YRExODj48P3bt3Z/Xq1Wc9//XXXyc2NhZfX1/atm3LO++84/b822+/jclkKvMoKCioytswjHMPMO0ALyIicv48jXzz5cuXM2XKFObNm0f//v158803GTJkCNu3b6d58+Zlzo+Li2PatGksWLCAnj17kpiYyMSJEwkJCWH48OGu8wIDA0lKSnJ7rY+PT5XfT3UrKrGxIzUH0AwwERGRijA0AM2ePZvx48czYcIEAObOnctXX31FXFwcM2fOLHP+kiVL+Pvf/86oUaMAaNmyJT/++CMvvviiWwAymUw0adLkvOsoLCyksLDQ9X1OTs6F3lK12nkolyKrjUAfT5o39DO6HBERkVrDsC6woqIiNm7cyKBBg9yODxo0iHXr1pX7msLCwjItOb6+viQmJlJcXOw6dvz4cVq0aEFUVBTDhg3j119/PWstM2fOJCgoyPVo1qzZBd5V9dpyyvgfk0kDoEVERM6XYQEoMzMTq9VKeHi42/Hw8HDS09PLfc3gwYN566232LhxI3a7nQ0bNhAfH09xcTGZmZkAtGvXjrfffpsVK1bw/vvv4+PjQ//+/dm1a9cZa5k2bRrZ2dmux4EDByrvRquQcwsMrQAtIiJSMYZ2gQFlWi7sdvsZWzOmT59Oeno6ffr0wW63Ex4eztixY3nppZcwm80A9OnThz59+rhe079/f7p168arr77KK6+8Uu51vb298fb2rqQ7qj7OFqAuCkAiIiIVYlgLUGhoKGazuUxrT0ZGRplWISdfX1/i4+PJz89n3759JCcnEx0dTUBAAKGhoeW+xsPDg549e561Bag2Kii2kpSeC0AnTYEXERGpEMMCkJeXF927dychIcHteEJCAv369Tvray0WC1FRUZjNZpYtW8awYcPw8Cj/Vux2O5s2bSIiIqLSaq8Jfk/PpcRmp5G/F5FBdW+Gm4iISFUytAts6tSpjBkzhh49etC3b1/mz59PcnIykyZNAhxjc1JSUlxr/ezcuZPExER69+7NsWPHmD17Ntu2bWPx4sWuaz7zzDP06dOHSy65hJycHF555RU2bdrE66+/bsg9VpUtB7MAx/gfDYAWERGpGEMD0KhRozhy5AgzZswgLS2Njh07snLlSlq0aAFAWloaycnJrvOtViuzZs0iKSkJi8XCwIEDWbduHdHR0a5zsrKyuPPOO0lPTycoKIhLL72UH374gV69elX37VUp1wwwLYAoIiJSYSa73W43uoiaJicnh6CgILKzswkMDDS6nHINnvMDSYdyWXB7D65pX/6YKRERkfqkIp/fhm+FIRWXX1TCrgzHAGitAC0iIlJxCkC10PbUHGx2CA/0JjxQA6BFREQqSgGoFnKO/+nUNNjYQkRERGopBaBayDkDTN1fIiIiF0YBqBbaklLaAqQAJCIickEUgGqZ3IJi9h7OA6CTpsCLiIhcEAWgWmZbSg4ATYN9CW1Q+/YvExERqQkUgGoZ1w7wav0RERG5YApAtYxrBehmCkAiIiIXSgGoltma4twCI9jYQkRERGoxBaBaJCu/iP1H8gF1gYmIiFwMBaBaxNn606KRH0F+FoOrERERqb0UgGqRkytAq/VHRETkYigA1SJbnQOgtQCiiIjIRVEAqkWcXWDaA0xEROTiKADVEpnHC0nJOoHJBB2bBhpdjoiISK2mAFRLOFt/Wob6E+CjAdAiIiIXQwGolthywDn+J9jYQkREROoABaBaQltgiIiIVB4FoFpii2aAiYiIVBoFoFrgUE4BGbmFeJigfaQGQIuIiFwsBaBawNn6c0lYAH5engZXIyIiUvspANUCWw9mAer+EhERqSwKQLXAlhSN/xEREalMCkA1nN1uP7kHmKbAi4iIVAoFoBouJesER/OK8PQw0a5JgNHliIiI1AkKQDWccwPUtk0C8LGYDa5GRESkblAAquE0/kdERKTyKQDVcM4WIO0ALyIiUnkUgGowxwDoLEAtQCIiIpVJAagG238kn5yCErw8PWgTrgHQIiIilUUBqAZzjv+JjQjEy1O/KhERkcqiT9UazLUCtHaAFxERqVQKQDXYyQUQFYBEREQqkwJQDWWz2dmmKfAiIiJVQgGohtqbmUdekRUfiwetGzcwuhwREZE6RQGohtqakgVAx8ggPM36NYmIiFQmfbLWUJsPaPyPiIhIVVEAqqG2avyPiIhIlVEAqoFKrDZ+S9UWGCIiIlVFAagG2n34OAXFNvy9zLQM9Te6HBERkTrH8AA0b948YmJi8PHxoXv37qxevfqs57/++uvExsbi6+tL27Zteeedd8547rJlyzCZTIwcObKSq65azvV/OjYNwsPDZHA1IiIidY+nkW++fPlypkyZwrx58+jfvz9vvvkmQ4YMYfv27TRv3rzM+XFxcUybNo0FCxbQs2dPEhMTmThxIiEhIQwfPtzt3P379/PQQw9x+eWXV9ftVBrnDvAa/yMiIlI1DG0Bmj17NuPHj2fChAnExsYyd+5cmjVrRlxcXLnnL1myhL///e+MGjWKli1bMnr0aMaPH8+LL77odp7VauXWW2/lmWeeoWXLltVxK5Vqi2sAdLCxhYiIiNRRhgWgoqIiNm7cyKBBg9yODxo0iHXr1pX7msLCQnx8fNyO+fr6kpiYSHFxsevYjBkzaNy4MePHjz+vWgoLC8nJyXF7GKWoxMaOVMf7qwVIRESkahgWgDIzM7FarYSHh7sdDw8PJz09vdzXDB48mLfeeouNGzdit9vZsGED8fHxFBcXk5mZCcDatWtZuHAhCxYsOO9aZs6cSVBQkOvRrFmzC7+xi7TzUC5FVhuBPp40b+hnWB0iIiJ1meGDoE0m90G+dru9zDGn6dOnM2TIEPr06YPFYmHEiBGMHTsWALPZTG5uLrfddhsLFiwgNDT0vGuYNm0a2dnZrseBAwcu+H4u1paDJ7u/zvRzEBERkYtj2CDo0NBQzGZzmdaejIyMMq1CTr6+vsTHx/Pmm29y6NAhIiIimD9/PgEBAYSGhrJlyxb27dvnNiDaZrMB4OnpSVJSEq1atSpzXW9vb7y9vSvx7i6ccwsMrQAtIiJSdQxrAfLy8qJ79+4kJCS4HU9ISKBfv35nfa3FYiEqKgqz2cyyZcsYNmwYHh4etGvXjq1bt7Jp0ybX4/rrr2fgwIFs2rTJ0K6t8+VqAWqqACQiIlJVDJ0GP3XqVMaMGUOPHj3o27cv8+fPJzk5mUmTJgGOrqmUlBTXWj87d+4kMTGR3r17c+zYMWbPns22bdtYvHgxAD4+PnTs2NHtPYKDgwHKHK+JCoqtJKXnAmoBEhERqUqGBqBRo0Zx5MgRZsyYQVpaGh07dmTlypW0aNECgLS0NJKTk13nW61WZs2aRVJSEhaLhYEDB7Ju3Tqio6MNuoPK9Xt6LiU2O438vWga7Gt0OSIiInWWyW63240uoqbJyckhKCiI7OxsAgMDq+1931m/jyf/7zeubNuYt//Wq9reV0REpC6oyOe34bPA5CSN/xEREakeCkA1iHMLjE5aAVpERKRKKQDVEPlFJezKcAyA1grQIiIiVUsBqIbYnpqDzQ5hAd6EB/qc+wUiIiJywRSAaohTV4AWERGRqqUAVENsde0Ar+4vERGRqqYAVENsPpgFaAFEERGR6qAAVAPkFhSz93AeAJ00BV5ERKTKKQDVANtScgBoGuxLaIOasSmriIhIXaYAVAO4doBX64+IiEi1UACqAba4FkBUABIREakOCkA1gHMGWBdNgRcREakWCkAGy8ovYv+RfEBdYCIiItVFAchgztafFo38CPKzGFyNiIhI/aAAZDDX+B+1/oiIiFQbBSCDbT2oFaBFRESqmwKQwZxdYJ2aBhtbiIiISD2iAGSgzOOFpGSdwGSCjk0DjS5HRESk3lAAMpCz9adlqD8BPhoALSIiUl0UgAy05YBz/E+wsYWIiIjUMwpABtIWGCIiIsZQADLQFs0AExERMYQCkEEO5RSQkVuIhwnaR2oAtIiISHVSADKIs/XnkrAA/Lw8Da5GRESkflEAMsjWg1mAdoAXERExggKQQba4doBXABIREaluCkAGsNvtJ/cA0xR4ERGRaqcAZICUrBMczSvC08NEuyYBRpcjIiJS7ygAGcC5AWrbJgH4WMwGVyMiIlL/KAAZwDn+R+v/iIiIGEMByADOFiDtAC8iImIMBaBq5hgAnQWoBUhERMQoCkDVLPloPjkFJXh5etAmXAOgRUREjKAAVM02l3Z/xUYE4uWpH7+IiIgR9AlczZwrQHfWDvAiIiKGUQCqZicXQFQAEhERMYoCUDWy2exs0xR4ERERwykAVaO9mXnkFVnxsXjQunEDo8sRERGptzyNLqA+OZRTQIifhZaNG+BpVvYUERExigJQNerfOpRfpl9DbmGJ0aWIiIjUa2qGqGYmk4lAH4vRZYiIiNRrCkAiIiJS7xgegObNm0dMTAw+Pj50796d1atXn/X8119/ndjYWHx9fWnbti3vvPOO2/Mff/wxPXr0IDg4GH9/f7p27cqSJUuq8hZERESkljF0DNDy5cuZMmUK8+bNo3///rz55psMGTKE7du307x58zLnx8XFMW3aNBYsWEDPnj1JTExk4sSJhISEMHz4cAAaNmzI448/Trt27fDy8uKzzz7jb3/7G2FhYQwePLi6b1FERERqIJPdbrcb9ea9e/emW7duxMXFuY7FxsYycuRIZs6cWeb8fv360b9/f15++WXXsSlTprBhwwbWrFlzxvfp1q0bQ4cO5Z///Od51ZWTk0NQUBDZ2dkEBgZW4I5ERETEKBX5/DasC6yoqIiNGzcyaNAgt+ODBg1i3bp15b6msLAQHx8ft2O+vr4kJiZSXFxc5ny73c6qVatISkpiwIABZ6ylsLCQnJwct4eIiIjUXYYFoMzMTKxWK+Hh4W7Hw8PDSU9PL/c1gwcP5q233mLjxo3Y7XY2bNhAfHw8xcXFZGZmus7Lzs6mQYMGeHl5MXToUF599VWuueaaM9Yyc+ZMgoKCXI9mzZpVzk2KiIhIjWT4IGiTyeT2vd1uL3PMafr06QwZMoQ+ffpgsVgYMWIEY8eOBcBsNrvOCwgIYNOmTfz8888899xzTJ06le+///6MNUybNo3s7GzX48CBAxd9XyIiIlJzGRaAQkNDMZvNZVp7MjIyyrQKOfn6+hIfH09+fj779u0jOTmZ6OhoAgICCA0NdZ3n4eFB69at6dq1Kw8++CA33nhjuWOKnLy9vQkMDHR7iIiISN1lWADy8vKie/fuJCQkuB1PSEigX79+Z32txWIhKioKs9nMsmXLGDZsGB4eZ74Vu91OYWFhpdQtIiIitZ+h0+CnTp3KmDFj6NGjB3379mX+/PkkJyczadIkwNE1lZKS4lrrZ+fOnSQmJtK7d2+OHTvG7Nmz2bZtG4sXL3Zdc+bMmfTo0YNWrVpRVFTEypUreeedd9xmmomIiEj9ZmgAGjVqFEeOHGHGjBmkpaXRsWNHVq5cSYsWLQBIS0sjOTnZdb7VamXWrFkkJSVhsVgYOHAg69atIzo62nVOXl4ed999NwcPHsTX15d27dqxdOlSRo0aVd23JyIiIjWUoesA1VRaB0hERKT2qRXrAImIiIgYxdAusJrK2SimBRFFRERqD+fn9vl0bikAlSM3NxdACyKKiIjUQrm5uQQFBZ31HI0BKofNZiM1NZWAgIAzLsp4oXJycmjWrBkHDhzQ+KIaQL+PmkW/j5pFv4+aR7+Ts7Pb7eTm5hIZGXnW5XFALUDl8vDwICoqqkrfQwsu1iz6fdQs+n3ULPp91Dz6nZzZuVp+nDQIWkREROodBSARERGpdxSAqpm3tzdPPfUU3t7eRpci6PdR0+j3UbPo91Hz6HdSeTQIWkREROodtQCJiIhIvaMAJCIiIvWOApCIiIjUOwpAIiIiUu8oAFWjefPmERMTg4+PD927d2f16tVGl1RvzZw5k549exIQEEBYWBgjR44kKSnJ6LIEx+/GZDIxZcoUo0up11JSUrjtttto1KgRfn5+dO3alY0bNxpdVr1UUlLCE088QUxMDL6+vrRs2ZIZM2Zgs9mMLq1WUwCqJsuXL2fKlCk8/vjj/Prrr1x++eUMGTKE5ORko0url/73v/9xzz338OOPP5KQkEBJSQmDBg0iLy/P6NLqtZ9//pn58+fTuXNno0up144dO0b//v2xWCx88cUXbN++nVmzZhEcHGx0afXSiy++yBtvvMFrr73Gjh07eOmll3j55Zd59dVXjS6tVtM0+GrSu3dvunXrRlxcnOtYbGwsI0eOZObMmQZWJgCHDx8mLCyM//3vfwwYMMDocuql48eP061bN+bNm8ezzz5L165dmTt3rtFl1UuPPvooa9euVSt1DTFs2DDCw8NZuHCh69gNN9yAn58fS5YsMbCy2k0tQNWgqKiIjRs3MmjQILfjgwYNYt26dQZVJafKzs4GoGHDhgZXUn/dc889DB06lKuvvtroUuq9FStW0KNHD2666SbCwsK49NJLWbBggdFl1VuXXXYZq1atYufOnQBs3ryZNWvWcN111xlcWe2mzVCrQWZmJlarlfDwcLfj4eHhpKenG1SVONntdqZOncpll11Gx44djS6nXlq2bBm//PILP//8s9GlCLB3717i4uKYOnUqjz32GImJidx33314e3tz++23G11evfPII4+QnZ1Nu3btMJvNWK1WnnvuOW655RajS6vVFICqkclkcvvebreXOSbVb/LkyWzZsoU1a9YYXUq9dODAAe6//36+/vprfHx8jC5HAJvNRo8ePXj++ecBuPTSS/ntt9+Ii4tTADLA8uXLWbp0Ke+99x4dOnRg06ZNTJkyhcjISO644w6jy6u1FICqQWhoKGazuUxrT0ZGRplWIale9957LytWrOCHH34gKirK6HLqpY0bN5KRkUH37t1dx6xWKz/88AOvvfYahYWFmM1mAyusfyIiImjfvr3bsdjYWD766CODKqrf/vGPf/Doo48yevRoADp16sT+/fuZOXOmAtBF0BigauDl5UX37t1JSEhwO56QkEC/fv0Mqqp+s9vtTJ48mY8//phvv/2WmJgYo0uqt6666iq2bt3Kpk2bXI8ePXpw6623smnTJoUfA/Tv37/MshA7d+6kRYsWBlVUv+Xn5+Ph4f5xbTabNQ3+IqkFqJpMnTqVMWPG0KNHD/r27cv8+fNJTk5m0qRJRpdWL91zzz289957/N///R8BAQGu1rmgoCB8fX0Nrq5+CQgIKDP2yt/fn0aNGmlMlkEeeOAB+vXrx/PPP8/NN99MYmIi8+fPZ/78+UaXVi8NHz6c5557jubNm9OhQwd+/fVXZs+ezbhx44wurVbTNPhqNG/ePF566SXS0tLo2LEjc+bM0ZRrg5xp7NWiRYsYO3Zs9RYjZVx55ZWaBm+wzz77jGnTprFr1y5iYmKYOnUqEydONLqseik3N5fp06fzySefkJGRQWRkJLfccgtPPvkkXl5eRpdXaykAiYiISL2jMUAiIiJS7ygAiYiISL2jACQiIiL1jgKQiIiI1DsKQCIiIlLvKACJiIhIvaMAJCIiIvWOApCIiIjUOwpAIiLnwWQy8emnnxpdhohUEgUgEanxxo4di8lkKvO49tprjS5NRGopbYYqIrXCtddey6JFi9yOeXt7G1SNiNR2agESkVrB29ubJk2auD1CQkIAR/dUXFwcQ4YMwdfXl5iYGD788EO312/dupU//elP+Pr60qhRI+68806OHz/udk58fDwdOnTA29ubiIgIJk+e7PZ8ZmYmf/7zn/Hz8+OSSy5hxYoVVXvTIlJlFIBEpE6YPn06N9xwA5s3b+a2227jlltuYceOHQDk5+dz7bXXEhISws8//8yHH37IN9984xZw4uLiuOeee7jzzjvZunUrK1asoHXr1m7v8cwzz3DzzTezZcsWrrvuOm699VaOHj1arfcpIpXELiJSw91xxx12s9ls9/f3d3vMmDHDbrfb7YB90qRJbq/p3bu3/a677rLb7Xb7/Pnz7SEhIfbjx4+7nv/888/tHh4e9vT0dLvdbrdHRkbaH3/88TPWANifeOIJ1/fHjx+3m0wm+xdffFFp9yki1UdjgESkVhg4cCBxcXFuxxo2bOj6um/fvm7P9e3bl02bNgGwY8cOunTpgr+/v+v5/v37Y7PZSEpKwmQykZqaylVXXXXWGjp37uz62t/fn4CAADIyMi70lkTEQApAIlIr+Pv7l+mSOheTyQSA3W53fV3eOb6+vud1PYvFUua1NputQjWJSM2gMUAiUif8+OOPZb5v164dAO3bt2fTpk3k5eW5nl+7di0eHh60adOGgIAAoqOjWbVqVbXWLCLGUQuQiNQKhYWFpKenux3z9PQkNDQUgA8//JAePXpw2WWX8e6775KYmMjChQsBuPXWW3nqqae44447ePrppzl8+DD33nsvY8aMITw8HICnn36aSZMmERYWxpAhQ8jNzWXt2rXce++91XujIlItFIBEpFb48ssviYiIcDvWtm1bfv/9d8AxQ2vZsmXcfffdNGnShHfffZf27dsD4Ofnx1dffcX9999Pz5498fPz44YbbmD27Nmua91xxx0UFBQwZ84cHnroIUJDQ7nxxhur7wZFpFqZ7Ha73egiREQuhslk4pNPPmHkyJFGlyIitYTGAImIiEi9owAkIiIi9Y7GAIlIraeefBGpKLUAiYiISL2jACQiIiL1jgKQiIiI1DsKQCIiIlLvKACJiIhIvaMAJCIiIvWOApCIiIjUOwpAIiIiUu/8P8x8k3LxBL/YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot training and validation accuracy over epochs\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f391a-2af7-4fbb-8ceb-d8db936e0dd4",
   "metadata": {},
   "source": [
    "## TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22e4f8-cee8-4711-9542-e8791d35aaa3",
   "metadata": {},
   "source": [
    "### 1. Present an overview of the AlexNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86953fe3-de2c-445a-a8ba-bf3884fc5ae6",
   "metadata": {},
   "source": [
    "Ans--> AlexNet is a deep convolutional neural network (CNN) architecture that played a pivotal role in advancing the field of computer vision and deep learning. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, marking a significant milestone in the development of deep learning models for image classification.\n",
    "\n",
    "Here's an overview of the AlexNet architecture:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   The network takes in RGB images of size 227x227 pixels as inputs. These images are preprocessed by subtracting the mean pixel value across the dataset from each channel.\n",
    "\n",
    "2. **Convolutional Layers:**\n",
    "   AlexNet consists of five convolutional layers, each followed by a max-pooling layer. These convolutional layers are designed to extract hierarchical features from the input images. The convolutional kernels used have varying sizes, with smaller kernels in the earlier layers and larger kernels in the deeper layers.\n",
    "\n",
    "3. **Activation Function:**\n",
    "   Rectified Linear Units (ReLU) are used as activation functions after each convolutional layer. ReLU introduces non-linearity to the model and helps in overcoming the vanishing gradient problem.\n",
    "\n",
    "4. **Max-Pooling Layers:**\n",
    "   After each of the first two convolutional layers, a max-pooling layer is applied to reduce the spatial dimensions of the feature maps while retaining important features. This helps in reducing the computational load and prevents overfitting.\n",
    "\n",
    "5. **Normalization Layers:**\n",
    "   Local Response Normalization (LRN) layers were introduced to the network. These layers aim to enhance the model's generalization capabilities by normalizing the responses of neighboring neurons.\n",
    "\n",
    "6. **Fully Connected Layers:**\n",
    "   AlexNet has three fully connected layers towards the end of the architecture. The first two fully connected layers each consist of 4096 neurons. These layers serve as high-level feature extractors and are followed by ReLU activation functions. The last fully connected layer is the output layer, containing as many neurons as the number of classes in the dataset. It uses a softmax activation function to produce class probabilities.\n",
    "\n",
    "7. **Dropout:**\n",
    "   Dropout layers are applied after the fully connected layers to prevent overfitting. Dropout randomly deactivates a certain fraction of neurons during training, making the network more robust and less likely to rely on specific neurons.\n",
    "\n",
    "8. **Output Layer:**\n",
    "   The output layer produces class probabilities for the given input image using the softmax activation function. The class with the highest probability is considered the predicted class for the input image.\n",
    "\n",
    "9. **Training and Optimization:**\n",
    "   AlexNet was trained using the stochastic gradient descent (SGD) optimization algorithm. It utilized techniques such as data augmentation (randomly transforming input images) and dropout to improve generalization and mitigate overfitting.\n",
    "\n",
    "In summary, AlexNet's architecture introduced several groundbreaking concepts like deep convolutional networks, ReLU activations, and dropout, which laid the foundation for modern deep learning in computer vision. Its success in the ImageNet competition demonstrated the potential of deep neural networks for image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c38adeb-008c-41e0-ae61-e81f67563324",
   "metadata": {},
   "source": [
    "### 2. Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c335c-6080-4e06-9465-615920d4e595",
   "metadata": {},
   "source": [
    "Ans--> AlexNet introduced several architectural innovations that played a crucial role in its breakthrough performance and subsequent advancements in deep learning. These innovations helped address various challenges in training deep neural networks and significantly improved their ability to extract meaningful features from images. Here are the key architectural innovations introduced in AlexNet:\n",
    "\n",
    "1. **Deep Convolutional Layers:**\n",
    "   AlexNet was one of the first deep convolutional neural networks (CNNs) that demonstrated the power of multiple layers of convolutions. Deep layers allowed the network to learn complex hierarchical features, capturing both low-level features like edges and textures as well as high-level features like object parts and structures.\n",
    "\n",
    "2. **ReLU Activation Function:**\n",
    "   AlexNet adopted the Rectified Linear Unit (ReLU) activation function instead of traditional activation functions like sigmoid or tanh. ReLU activations significantly accelerate convergence during training and mitigate the vanishing gradient problem. This choice of activation function enabled faster and more effective learning.\n",
    "\n",
    "3. **Large Training Dataset:**\n",
    "   AlexNet was trained on the ImageNet dataset, which was much larger than datasets used in previous studies. This allowed the network to learn diverse features and generalize better to real-world images.\n",
    "\n",
    "4. **Data Augmentation:**\n",
    "   To further enhance generalization, AlexNet employed data augmentation techniques during training. This involved applying random transformations to the input images, such as cropping, flipping, and rotating. Data augmentation increased the diversity of the training data and made the network more robust to variations in real-world images.\n",
    "\n",
    "5. **Dropout Regularization:**\n",
    "   Dropout is a regularization technique introduced in AlexNet. During training, dropout randomly deactivates a certain fraction of neurons in each layer, preventing the network from relying too heavily on any particular set of neurons. This reduces overfitting and improves generalization, leading to better performance on unseen data.\n",
    "\n",
    "6. **Parallelization and GPUs:**\n",
    "   AlexNet's architecture was designed with parallelization in mind. It was one of the first models to take full advantage of Graphics Processing Units (GPUs) for training. GPUs sped up the training process significantly, enabling faster experimentation and model development.\n",
    "\n",
    "7. **Local Response Normalization (LRN):**\n",
    "   AlexNet introduced LRN layers after convolutional layers. While this concept is less commonly used in modern architectures, it was believed to provide a form of lateral inhibition, enhancing the contrast between activated neurons and contributing to the network's generalization.\n",
    "\n",
    "8. **Large Convolutional Kernels:**\n",
    "   The first convolutional layer of AlexNet used a large kernel size (11x11) with a stride of 4. This architecture choice allowed the network to capture larger-scale features in the initial layers while reducing the spatial dimensions of the feature maps.\n",
    "\n",
    "9. **Ensemble of Models:**\n",
    "   The winning solution of the ILSVRC 2012 competition was an ensemble of multiple AlexNet models. This approach further improved the performance by combining predictions from different instances of the same architecture.\n",
    "\n",
    "Collectively, these architectural innovations introduced in AlexNet helped address issues related to overfitting, training convergence, and feature learning, ultimately leading to a breakthrough in image classification performance and paving the way for the subsequent development of more sophisticated deep learning architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3a37f-04f5-4b99-b773-4acef0c8debd",
   "metadata": {},
   "source": [
    "### 3. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c3846-9df4-4ac8-a9fd-cd1b53eb9b40",
   "metadata": {},
   "source": [
    "Ans--> In AlexNet, convolutional layers, pooling layers, and fully connected layers play distinct roles in the overall architecture, each contributing to the network's ability to extract features from input images and make accurate predictions. Let's discuss the role of each of these types of layers in more detail:\n",
    "\n",
    "1. **Convolutional Layers:**\n",
    "   Convolutional layers are the core building blocks of convolutional neural networks (CNNs). In AlexNet, there are five convolutional layers. These layers are responsible for detecting various features present in the input image, such as edges, textures, and more complex patterns. The deeper convolutional layers learn to capture increasingly complex and high-level features, enabling the network to recognize specific object parts and structures.\n",
    "\n",
    "   Each convolutional layer applies multiple filters (also known as kernels) to the input feature maps. These filters slide over the input, performing element-wise multiplications and summations to produce feature maps that highlight specific patterns. The application of ReLU activation functions after each convolutional layer introduces non-linearity, allowing the network to learn more complex relationships between features.\n",
    "\n",
    "2. **Pooling Layers:**\n",
    "   Pooling layers, specifically max-pooling in AlexNet, serve to reduce the spatial dimensions of the feature maps while preserving important features. Max-pooling involves selecting the maximum value within a small region (usually 2x2) of the feature map and discarding the rest. This downsampling operation reduces the computational load and the number of parameters in the subsequent layers, making the network more efficient.\n",
    "\n",
    "   Max-pooling also introduces a degree of translation invariance, meaning that the network can still recognize patterns and features even when they're slightly shifted in the input image. This property helps the network generalize better to variations in object positions.\n",
    "\n",
    "3. **Fully Connected Layers:**\n",
    "   The fully connected layers in AlexNet are inspired by traditional neural networks. These layers are responsible for high-level feature extraction and making final class predictions. AlexNet has three fully connected layers, with the first two layers each containing 4096 neurons. These layers process the features learned by the preceding convolutional and pooling layers to make abstract representations of the input image.\n",
    "\n",
    "   The fully connected layers are connected to every neuron in the previous layer, forming a dense network. ReLU activations are applied after the first two fully connected layers, introducing non-linearity. The final fully connected layer, also known as the output layer, consists of neurons equal to the number of classes in the dataset. It employs a softmax activation function to produce class probabilities, determining the predicted class for the input image.\n",
    "\n",
    "In summary, convolutional layers perform feature extraction by applying convolutional filters to the input data. Pooling layers downsample the feature maps to reduce dimensions and introduce translation invariance. Fully connected layers process high-level features and make class predictions. The combination of these layers allows AlexNet to learn hierarchical features from images and make accurate image classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9aeab-5672-4b33-b044-cc97a55240c6",
   "metadata": {},
   "source": [
    "### 4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4172247-796c-4cbe-af0d-fa2df553e317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.56.2)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.33.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e9a4680-d10d-4cdc-9321-c5a38bab6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models,layers,datasets,optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4edcc97a-7cfa-4543-a98e-beeea19aab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, valid_images, train_labels, valid_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "valid_images = valid_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "valid_labels = to_categorical(valid_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3c9d6a8-5c32-478f-88bd-b4d9a7bb04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build AlexNet architecture\n",
    "def alexnet(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "    # Fully connected layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d090d25-b87e-41dd-8db2-90a5deb8b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_images.shape[1:]\n",
    "num_classes = 10\n",
    "model = alexnet(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba3fdf-47dc-45ff-818e-be3b0603ee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 41s 129ms/step - loss: 2.3029 - accuracy: 0.0998 - val_loss: 2.3033 - val_accuracy: 0.0979\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 37s 118ms/step - loss: 2.3027 - accuracy: 0.1005 - val_loss: 2.3030 - val_accuracy: 0.0979\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 37s 118ms/step - loss: 2.3030 - accuracy: 0.1005 - val_loss: 2.3029 - val_accuracy: 0.0933\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 37s 119ms/step - loss: 2.3028 - accuracy: 0.1001 - val_loss: 2.3026 - val_accuracy: 0.1040\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 37s 120ms/step - loss: 2.3023 - accuracy: 0.1015 - val_loss: 2.3005 - val_accuracy: 0.1017\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 37s 120ms/step - loss: 2.2638 - accuracy: 0.1538 - val_loss: 2.1129 - val_accuracy: 0.1838\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 37s 119ms/step - loss: 2.0441 - accuracy: 0.1903 - val_loss: 1.9418 - val_accuracy: 0.2104\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 37s 120ms/step - loss: 1.9357 - accuracy: 0.2128 - val_loss: 1.8880 - val_accuracy: 0.2293\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 37s 119ms/step - loss: 1.8804 - accuracy: 0.2526 - val_loss: 1.8008 - val_accuracy: 0.3005\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 37s 119ms/step - loss: 1.7555 - accuracy: 0.3200 - val_loss: 1.7025 - val_accuracy: 0.3402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f32db150d60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.compile(optimizer=optimizers.SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_data=(valid_images, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d275c8e-8334-445e-a042-dcf30008622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4f6c1-c476-4bc2-a52d-f0450717866a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
